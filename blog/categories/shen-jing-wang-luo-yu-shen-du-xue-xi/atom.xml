<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 神经网络与深度学习 | EdmondFrank's 时光足迹]]></title>
  <link href="https://edmondfrank.github.io/blog/categories/shen-jing-wang-luo-yu-shen-du-xue-xi/atom.xml" rel="self"/>
  <link href="https://edmondfrank.github.io/"/>
  <updated>2018-02-15T08:53:17+08:00</updated>
  <id>https://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[传统机器学习走向神经网络]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/14/chuan-tong-ji-qi-xue-xi-zou-xiang-shen-jing-wang-luo/"/>
    <updated>2017-12-14T23:43:13+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/14/chuan-tong-ji-qi-xue-xi-zou-xiang-shen-jing-wang-luo</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="传统机器学习走向神经网络">传统机器学习走向神经网络</h1></p>

<h2 id="神经网络的基本结构">神经网络的基本结构</h2>




<p>首先我们先来看一下最基础的神经网络结构： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgm7cftunj20kf0ckaep.jpg" alt="" title=""></p>




<p>由上图的结构可以看出，这个神经网络具有三层，其中输入层不计。而中间的橙色层则为两层隐藏层，最右的蓝色层为输出层。输入从最左边的输入层进行输入，然后经过两次隐藏层和激活函数之后进行输出，这样我们可以把这个神经网络简单地表示成一下的式子： <br>
<script type="math/tex; mode=display" id="MathJax-Element-82">Y_{out} = W_iX_{in}+B</script> <br>
W为X的权重，而B为函数的偏置。 <br>
其中，偏置值B的存在有利于打破数据对称的局面，使得神经网络可以应用在非对称的数据之上。</p>




<h2 id="神经网络的基本算法">神经网络的基本算法</h2>




<p>前向传导：前向传导的思想比较简单，下面的一张图足以概括它的主要思想。 <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgp39x2ltj20oh0gydi9.jpg" alt="" title=""></p>




<p>反向传播：反向传播的方法其实也比较简单，其主要思想是涉及求偏导，以及链式求导法则。 <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgp6qj2wlj20mj0i441b.jpg" alt="" title=""></p>




<p>梯度下降：梯度下降法是一个最优化算法，通常也称为最速下降法。最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现已不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。</p>




<h2 id="朴素贝叶斯和神经网络">朴素贝叶斯和神经网络</h2>




<p>首先朴素贝叶斯算法的原始形式可以表达成以下的形式： <br>
<script type="math/tex" id="MathJax-Element-1467">G(x)=arg\  max\ p(y)  
\prod\limits^n_{i=1}p(x_i|y)^{x_i}</script></p>




<p>除此之外，该算法还有一下特点： <br>
<script type="math/tex" id="MathJax-Element-1468">x_i只有0，1两种取值</script> <br>
<script type="math/tex" id="MathJax-Element-1469">x_i取1意味着x_i对应了的特征“出现了”</script>  <br>
<script type="math/tex" id="MathJax-Element-1470">x_i取0意味着x_i对应了的特征“没出现”</script></p>




<p>这样转换成矩阵的形式时，我们可以采用独热编码亦称One-hot Encode。 <br>
独热编码：</p>




<p>解决了分类标签的问题，那么我们又该怎样用神经网络的线性模型形式来表达贝叶斯公式中概率相乘的情况呢？</p>




<p>没错，就是使用对数函数。根据对数函数的性质<script type="math/tex" id="MathJax-Element-1471">log_2X+log_2Y=log_2XY</script>,我们就可以通过对数变换，将乘法转换成加法的形式，我们可以把上面的朴素贝叶斯公式改写成： <br>
<script type="math/tex" id="MathJax-Element-1472">G(x)=arg\ max\ log(y)+\sum\limits^n_{i=1}x_ilog\ p(x_i|y)</script></p>




<p>那么我们就可以用退化成线性模型的神经网络来实现朴素贝叶斯模型。</p>




<h3 id="核心实现">核心实现</h3>


<pre><code class="python"># 独热化处理部分
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
x_train = enc.fit_transform(x_train).toarray()
x_test = enc.transform(x_test).toarray()

## .....篇幅有限,此处省略其余代码

# NaiveBayes -&gt; NN 权值转换部分
class NB2NN(TransformationBase):
    def __init__(self, *args, **kwargs):
        super(NB2NN, self).__init__(*args, **kwargs)
        self._name_appendix = "NaiveBayes"
        self.model_param_settings.setdefault("activations", None)

    def _transform(self):
        self.hidden_units = []
        x, y, x_test, y_test = self._get_all_data()
        nb = MultinomialNB()
        nb.fit(x, y)
        self._print_model_performance(nb, "Naive Bayes", x, y, x_test, y_test)
        self._transform_ws = [nb.feature_log_prob_.T]
        self._transform_bs = [nb.class_log_prior_]
</code></pre>

<h2 id="决策树贝叶斯和神经网络">决策树贝叶斯和神经网络</h2>




<p>首先，决策树的原理主要就是通过数据信息熵的变化来选择当前的最优分类点，然后从根开始一步一步扩展成树。而实质上，最后成功构建出来的决策树，其从根节点开始到每个分类叶子节点的路径对应的都是一组高维空间上的超平面组合。决策树的分类也就是用一组超平面去划分数据空间，使得最后剩下一个唯一确定的标识。</p>




<p>知道决策树的本质之后，我们就可以用这样的方法来将决策树算法迁移到神经网络上： <br>
* 第一个隐藏层表达决策树的中间节点所对应的超平面 <br>
* 第二个隐藏层表达各个决策的路径 <br>
* 第二个隐藏层和输出层之间的权值矩阵表达各个叶节点</p>




<h3 id="核心实现-1">核心实现</h3>


<pre><code class="python">## 因为决策树到神经网络的转换较为复杂,此处仅贴出核心代码
class DT2NN(TransformationBase):
    def __init__(self, *args, **kwargs):
        super(DT2NN, self).__init__(*args, **kwargs)
        self._name_appendix = "DTree"
        self.model_param_settings.setdefault("activations", ["sign", "one_hot"])

    def _transform(self):
        x, y, x_test, y_test = self._get_all_data()
        tree = DecisionTreeClassifier()
        tree.fit(x, y)
        self._print_model_performance(tree, "Decision Tree", x, y, x_test, y_test)

        tree_structure = export_structure(tree)
        n_leafs = sum([1 if pair[1] == -1 else 0 for pair in tree_structure])
        n_internals = n_leafs - 1

        print("Internals : {} ; Leafs : {}".format(n_internals, n_leafs))

        b = np.zeros(n_internals, dtype=np.float32)
        w1 = np.zeros([x.shape[1], n_internals], dtype=np.float32)
        w2 = np.zeros([n_internals, n_leafs], dtype=np.float32)
        w3 = np.zeros([n_leafs, self.n_class], dtype=np.float32)
        node_list = []
        node_sign_list = []
        node_id_cursor = leaf_id_cursor = 0
        max_route_length = 0
        self.hidden_units = [n_internals, n_leafs]

        for depth, feat_dim, rs in tree_structure:
            if feat_dim != -1:
                if depth == len(node_list):
                    node_sign_list.append(-1)
                    node_list.append([node_id_cursor, feat_dim, rs])
                    w1[feat_dim, node_id_cursor] = 1
                    b[node_id_cursor] = -rs
                    node_id_cursor += 1
                else:
                    node_list = node_list[:depth + 1]
                    node_sign_list = node_sign_list[:depth] + [1]
            else:
                valid_nodes = set()
                local_sign_list = node_sign_list[:]
                for i, ((node_id, node_dim, node_threshold), node_sign) in enumerate(
                    zip(node_list, node_sign_list)
                ):
                    valid_nodes.add((node_id, node_sign))
                    if i &gt;= 1:
                        for j, ((local_id, local_dim, local_threshold), local_sign) in enumerate(zip(
                            node_list[:i], local_sign_list[:i]
                        )):
                            if node_sign == local_sign and node_dim == local_dim:
                                if (
                                    (node_sign == -1 and node_threshold &lt; local_threshold) or
                                    (node_sign == 1 and node_threshold &gt; local_threshold)
                                ):
                                    local_sign_list[j] = 0
                                    valid_nodes.remove((local_id, local_sign))
                                    break
                for node_id, node_sign in valid_nodes:
                    w2[node_id, leaf_id_cursor] = node_sign / len(valid_nodes)
                max_route_length = max(max_route_length, len(valid_nodes))
                w3[leaf_id_cursor] = rs / np.sum(rs)
                leaf_id_cursor += 1

        w2 *= max_route_length
        self._transform_ws = [w1, w2, w3]
        self._transform_bs = [b]

#................ 篇幅有限,省略其余代码

# DTree -&gt; NN
def export_structure(tree):
    tree = tree.tree_

    def recurse(node, depth):
        feature_dim = tree.feature[node]
        if feature_dim == _tree.TREE_UNDEFINED:
            yield depth, -1, tree.value[node]
        else:
            threshold = tree.threshold[node]
            yield depth, feature_dim, threshold
            yield from recurse(tree.children_left[node], depth + 1)
            yield depth, feature_dim, threshold
            yield from recurse(tree.children_right[node], depth + 1)

    return list(recurse(0, 0))
</code></pre>

<h2 id="模型改进">模型改进</h2>




<h3 id="对于朴素贝叶斯">对于朴素贝叶斯</h3>




<p>根据上述的原理和理论，我们可以将朴素贝叶斯和决策树转换成神经网络模型，但是转换之后是否存在意义呢？</p>




<p><strong>首先</strong>本身可以通过简单log对数转换成线性模型的朴素贝叶斯算法来说，其转换的步骤并不复杂，但却能够对朴素贝叶斯的独立假设进行一定的微调修正。</p>




<h3 id="对于决策树">对于决策树</h3>




<p>那么对于决策树来说，神经网络的介入可以对决策树的硬边界作一定的修正和“软化”作用。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习入门简介（二）]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/08/shen-du-xue-xi-ru-men-jian-jie-(er-)/"/>
    <updated>2017-12-08T12:52:08+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/08/shen-du-xue-xi-ru-men-jian-jie-(er-)</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="深度学习入门简介二">深度学习入门简介（二）</h1></p>

<h2 id="深度学习的三部曲">深度学习的三部曲</h2>




<h3 id="训练前的准备">训练前的准备</h3>




<p>1）训练数据 <br>
在训练一个深度学习的模型之前，我们首先需要准备的就是训练数据，若是图片的话其中就包括：图片的内容以及他的标签。 <br>
<strong>注：学习的分类目标也是包括在训练数据里面的</strong></p>




<p>2）学习目标 <br>
学习的目标往往就是一个二分类或者多分类问题。而对于最后的效果，我们需要达到当我们输入一个待预测或分类的值时，正确的结果应该对应那个最大概率的输出项。</p>




<p>3）损失函数 <br>
简单来说，深度学习的分类和回归的本质就是，找到一个使得在所有样本项上取得的误差值最小的函数。而预测值与真实值的误差我们可以通过他们之间的距离计算得出。</p>




<h3 id="最小化误差">最小化误差</h3>




<p>为了达到一个分类或预测准确的效果，我们就要找到一个网络中的对应的超参数<script type="math/tex" id="MathJax-Element-471">\theta</script>使得网络的预测与真实值的误差是最小的。其中一个简单而粗暴的方法就是：枚举法。但是这样做的效率显然非常的低效。为了能够更加优化地找到或者说是接近使得网络取得最小误差的超参数<script type="math/tex" id="MathJax-Element-472">\theta</script>我们可以采用<strong>梯度下降法</strong>，其根据预设的学习率不断更新权重的梯度来接近局部最优解。</p>




<p>其具体过程图如下所示： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm92vbwnztj20ps0em0ub.jpg" alt="" title=""></p>




<p><strong>梯度下降的缺点：</strong> <br>
由于梯度下降每次计算时都是随机选取一个开始点，再根据学习率来慢慢减小全局误差。这样一来，学习率的设定就十分重要了，过大的学习率容易越过最低点，而过小的学习率又使得误差降低的速度过慢，且过小的学习率也会造成学习过程中陷入局部最低点后无法跳出。但实际上由于精度误差的问题梯度下降永远无法到达真正意义上的全局最低点，即无法取得全局最有解。但在多次的迭代运算后一般可以达到一个可接受的损失误差的局部最优解。</p>




<p>具体图示如下： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm937sjcc5j20q40hcdn6.jpg" alt="" title=""></p>




<h3 id="反向传播">反向传播</h3>




<p><strong>反向传播算法</strong>：这是一种高效的计算权值梯度的方式。</p>




<p>有关算法的详细介绍可以参考：</p>




<p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/index.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/index.html</a></p>




<p>通常我们在使用流行框架来构建神经网络时，不用亲自考虑如何去计算和处理梯度值，框架的作者在实现框架时已经做好了相关处理。</p>




<h3 id="神经网络的理论">神经网络的理论</h3>




<p>根据 <a href="http://neuralnetworksanddeeplearning.com/chap4.html">A visual proof that neural nets can compute any function</a> 文章的描述任何的连续函数 f 都可以用一个隐藏层内有足够多的神经元的神经网络来近似。</p>




<p><strong>既然这样，为什么今天流行的是深度网络而不是广度网络呢？</strong></p>




<p>根据 <a href="https://www.microsoft.com/en-us/research/publication/conversational-speech-transcription-using-context-dependent-deep-neural-networks/">Seide, Frank, Gang Li, and Dong Yu. “Conversational Speech Transcription <br>
Using Context-Dependent Deep Neural Networks.” Interspeech. 2011. <br>
</a> 论文的研究，广度和深度网络对降低全局误差时的参数如下表所示：</p>




<p><img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm98wlbz8yj20np0g2gmz.jpg" alt="fat-vs-deep" title=""></p>




<p>根据上图的研究结果，我们可以发现使用多层的神经元能够更加容易近似一些函数，这其实就跟我们的电子电路中的逻辑电路类似，即便在电子电路中两层的逻辑门电路就可以实现任意的逻辑操作，但是使用多层的逻辑门电路可以更容易的构建一些逻辑操作。</p>




<h3 id="模块化">模块化</h3>




<p>深度学习中还有一个特点就是<strong>模块化</strong>，在一层层的网络层的堆叠中，每一层都会作为一个模块来学习数据。简单来说，深度学习的过程其实就是一个自动提取特征的过程。对于传统的机器学习而言，数据科学家通过特征工程，提取出数据的特征，再利用特征对数据进行建模以此达到分类预测的效果。深度学习通过各个神经元的加权组合以及反向传播的权值调整，使得整个网络的每一层都渐渐趋向稳定，且其稳定值能够在那个维度上进行部分数据的划分，简单来说就是一个区域性的能够对数据有所区分的特性。那随着各个神经层的共同作用使得深度学习在分类预测应用上效果显著。</p>




<p>最后，深度学习在图像分类的本质大概可以用以下这张图片概括： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm99ftf2vwj20ow0dsn0p.jpg" alt="" title=""></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习入门简介]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/02/shen-du-xue-xi-ru-men-jian-jie/"/>
    <updated>2017-12-02T17:17:32+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/02/shen-du-xue-xi-ru-men-jian-jie</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="深度学习入门简介">深度学习入门简介</h1></p>

<h2 id="背景">背景</h2>




<p><strong>深度学习</strong>的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。</p>




<h2 id="概念">概念</h2>




<p><strong>深度学习</strong>的概念由Hinton等人于2006年提出。基于深度置信网络(DBN)提出非监督贪心逐层训练算法，为解决深层结构相关的优化难题带来希望，随后提出多层自动编码器深层结构。此外Lecun等人提出的卷积神经网络是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能。</p>




<h2 id="原理">原理</h2>




<p><strong>深度学习</strong>是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。</p>




<p>（以上内容摘取自百度百科）</p>




<p><strong>个人理解：</strong>如果说机器学习是为了找出一个能够代表输入变量和输出变量的关系的函数的话；那么深度学习就是先根据输入和输出变量之间的关系，列出一系列能够代表他们之间关系的函数，然后再从这个函数集之中提取一个最优的函数。</p>




<h2 id="结构">结构</h2>




<h3 id="神经元">神经元</h3>




<p>随着神经网络的应用和深度学习在人工智能领域的大放异彩，很多人都说神经网络的是最成功的仿真模型。那么他的结构究竟是怎样子的呢？</p>




<p><img src="https://i.loli.net/2017/12/01/5a217a29cf015.png" alt="nn.png" title=""></p>




<p>一个简单的神经网咯函数（一般称作：神经元），就如上图所示。</p>




<p>他的主要执行过程：</p>




<blockquote>
  <p>多个输入a X 各自的权重w + 偏置值b =&gt; 激活函数 =&gt; 输出</p>
</blockquote>




<p>其中，在这个流程之中，我们可能比较迷惑的是那个激活函数Activation function。</p>




<p><strong>Activation Function：</strong>即激活函数，目前的常用的激活函数由挺多的，例如，Simmoid Function，tanh，relu等等。虽然形式上不同，但是他们大体的目的都是较为一致的，就是用来加入非线性因素的，因为线性模型的表达能力不够。</p>




<p><script type="math/tex; mode=display" id="MathJax-Element-1"> 如下所示的Sigmoid Function \\\sigma(z)=\frac{1}{1+e^{-z}}</script></p>




<p>同时，激活函数可以将非常大或非常小的数据映射到“逻辑空间”[-1,1]之间，这样映射过后的数据更适合在反向传播算法中进行梯度下降。</p>




<h3 id="连接方式">连接方式</h3>




<p>上面我们提及的仅仅是神经网络中的一个神经元，他是神经网络之中最基本的组成单位。但是如果要构建一个强大智能的神经网络，仅仅靠一个神经元是不行的。于是，我们便可以将多个神经元分层连接起来，这样才构成了我们所知道的神经网络。</p>




<p>既然，神经网络的构成本质就是神经元的连接，那么不同的连接方式就会形成不同的神经网络结构如全连接前馈网络，多层感知器，卷积神经网络，循环神经网络等等。</p>




<h2 id="全连接前馈网络">全连接前馈网络</h2>




<p>在众多的连接之间，全连接的前馈网络不仅较为简单，也是很多深层网络的基础。他的基本连接方式如下图片所示：</p>




<p><img src="https://i.loli.net/2017/12/02/5a2180e6120e2.png" alt="feedforward.png" title=""></p>




<p>其中，一般来说神经网络的第一层通常都是输入层，而最后一层便是输出层以及中间的都统一称作隐藏层。深度神经网络中的“深”便代表了这个网络中间有非常多的隐藏层。</p>




<h2 id="输出层">输出层</h2>




<p>通常，输出层一般为Softmax 层，并且其可以为任意值。在应用中，输出的结果通常用概率的形式表达，其具体形式如下图所示： <br>
<img src="https://i.loli.net/2017/12/02/5a218350cc19b.png" alt="output.png" title=""></p>




<p>那么，我们知道了神经网络的组成之后，我们要是想自己构建一个神经网络，我们又该如何确定神经网络的层数和每层的神经元的个数呢？</p>




<p><strong>就目前来说，</strong>并没有相当的严谨的理论来指导神经网络的构建。我们往往需要依靠直觉和训练测试结果的误差反馈来一步一步选择我们的层数和神经元数以达到要求的效果。</p>

]]></content>
  </entry>
  
</feed>
