<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 算法,python,knn | EdmondFrank's 时光足迹]]></title>
  <link href="http://edmondfrank.github.io/blog/categories/suan-fa-python-knn/atom.xml" rel="self"/>
  <link href="http://edmondfrank.github.io/"/>
  <updated>2017-08-23T12:21:44+08:00</updated>
  <id>http://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[K-近邻算法-Part2]]></title>
    <link href="http://edmondfrank.github.io/blog/2017/08/23/k-jin-lin-suan-fa-part2/"/>
    <updated>2017-08-23T12:19:17+08:00</updated>
    <id>http://edmondfrank.github.io/blog/2017/08/23/k-jin-lin-suan-fa-part2</id>
    <content type="html"><![CDATA[<h1>K-近邻算法-Part2</h1>

<h2>使用交叉验证来调整k值</h2>

<p>通常来说,一个最优的KNN模型其k参数所对应的预估错误率应该是最低的。因此，在选定模型k值的时候应该反复尝试不同的k值在预估上的效果，对比其错误率。初学者在这里为了降低模型的误差通常会将全部样本数据也作为训练集一起代入模型进行训练。虽然这种做法在训练时确实能够有效降低误差，对现有数据进行更好的拟合。但是，同时带来的后果是：我们会将数据的各种无法避免的真实误差，如测量误差，抽样误差等也训练进了我们的模型之中，使得训练出来的模型在新数据或未知数据上的预估效果特别差，这种现象也被称为<strong>过拟合（overfitting）</strong>。</p>

<p>为了降低预估的错误率以及避免过拟合现象的发生，我们可以在某种意义下将<strong>原始数据(dataset)</strong>进行分组,一部分做为<strong>训练集(train set)</strong>,另一部分做为<strong>验证集(validation set or test set)</strong>，首先用训练集对分类器进行训练,再利用验证集来测试训练得到的<strong>模型(model)</strong>,以此来做为评价分类器的性能指标。这种方法也就是所谓的<strong>交叉验证（Cross Validation）</strong>。</p>

<p>而在交叉验证中，<strong>K折交叉验证（k-fold cross validation）</strong>是比较常用的。其主要思想是：初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</p>

<p><img src="https://kevinzakka.github.io/assets/k_fold_cv.jpg" alt="enter image description here" /></p>

<p>为了更好的理解k折交叉验证，我们继续沿用part1部分的训练数据，使用10折交叉验证的方法来调整我们的k值。</p>

<pre><code class="python">
import matplotlib.pyplot as plt
from sklearn.cross_validation import cross_val_score
# creating odd list of K for KNN
myList = list(range(1,50))

# subsetting just the odd ones
neighbors = list(filter(lambda x: x % 2 != 0, myList))

# empty list that will hold cv scores
cv_scores = []

# perform 10-fold cross validation
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# changing to misclassification error
MSE = [1 - x for x in cv_scores]

# determining best k
optimal_k = neighbors[MSE.index(min(MSE))]
print ("The optimal number of neighbors is %d" % optimal_k)

# plot misclassification error vs k
plt.plot(neighbors, MSE)
plt.xlabel('Number of Neighbors K')
plt.ylabel('Misclassification Error')
plt.show()
</code></pre>

<p>在上面的程序中，我们在1-50的奇数中选取k值，并根据k值训练模型进行预估验证。最后根据不同k值训练出去的模型的<strong>均方误差（Mean Squared Error, MSE）</strong>作出折线图，并求出使得MSE最小的最优k值。</p>

<blockquote><p>输出结果:</p>

<p>The optimal number of neighbors is 7
<img src="https://kevinzakka.github.io/assets/cv_knn.png" alt="enter image description here" /></p></blockquote>

<p>由此我们可以得出：在这个模型中，10折交叉验证告诉我们最优的k值是7。</p>

<h2>尝试自己实现KNN算法</h2>

<p>到目前为止，我们都是调用sklearn库中的KNN来完成分类任务。那么，下面我们来尝试自己实现一个简单的KNN算法并用它来分类我们之前的数据。</p>

<p>经过上一篇文章的介绍，我们可以知道KNN算法的关键是计算出新的<strong>待分类数据</strong>与现有的样本数据的<strong>“距离”</strong>，而其中较为常用的还是<strong>欧式距离（euclidean distance ）</strong>。然后提取出k个最相邻的点，并根据他们大多数点的分类属性来给待分类点进行归类。</p>

<p>因此核心计算代码我们可以这样写：</p>

<pre><code class="python">def predict(X_train, y_train, x_test, k):
    # create list for distances and targets
    distances = []
    targets = []

    for i in range(len(X_train)):
        # first we compute the euclidean distance
        distance = np.sqrt(np.sum(np.square(x_test - X_train[i, :])))
        # add it to list of distances
        distances.append([distance, i])

    # sort the list
    distances = sorted(distances)

    # make a list of the k neighbors' targets
    for i in range(k):
        index = distances[i][1]
        targets.append(y_train[index])

    # return most common target
    return Counter(targets).most_common(1)[0][0]
</code></pre>

<p>在上面的代码中，我们首先创建一个保存距离的数组，并在存储完计算出的待测点与各样本点的距离后对数组进行升序排列，然后取出前k个最接近待测点的样本点，返回其出现最多的分类标签。</p>

<p>然后接下来让我继续完成整个KNN算法。</p>

<pre><code class="python">def kNearestNeighbor(X_train, y_train, X_test, predictions, k):

    # loop over all observations
    for i in range(len(X_test)):
        predictions.append(predict(X_train, y_train, X_test[i, :], k))
</code></pre>

<p>使用我们上面得出最优的 k = 7作为参数生成模型并进行预估。</p>

<pre><code class="python"># making our predictions 
from sklearn.metrics import accuracy_score
predictions = []

kNearestNeighbor(X_train, y_train, X_test, predictions, 7)

# transform the list into an array
predictions = np.asarray(predictions)
#print(y_test,predictions)
# evaluating accuracy
#for i in range(predictions.size):
#    print(predictions.tolist()[i],list(y_test)[i])
accuracy = accuracy_score(y_test, predictions)
print('\nThe accuracy of our classifier is %d%%' % int(accuracy*100))
</code></pre>

<blockquote><p>输出结果：
The accuracy of our classifier is 98%</p></blockquote>

<p>到此，我们基本已经完成了一个类似于sklearn库中的KNN算法了，并且还有不错的准确率。</p>

<h2>小结</h2>

<p>最后，KNN算法介绍性文章到这就结束了。我们来总结一下KNN算法的优点与不足。</p>

<h4>优点</h4>

<ul>
<li>易于理解</li>
<li>无需训练</li>
<li>容易迁移至多分类情况</li>
</ul>


<h4>不足</h4>

<ul>
<li>计算量大，时间复杂度随数据规模增大而增大</li>
<li>分类情况容易受高频分类影响</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-近邻算法-Part1]]></title>
    <link href="http://edmondfrank.github.io/blog/2017/08/14/k-jin-lin-suan-fa-part1/"/>
    <updated>2017-08-14T23:48:56+08:00</updated>
    <id>http://edmondfrank.github.io/blog/2017/08/14/k-jin-lin-suan-fa-part1</id>
    <content type="html"><![CDATA[<h1>K-近邻算法-Part1</h1>

<h2>概述</h2>

<p>K-近邻算法，即K-近邻分类算法，简称KNN其通过采用测量不同的特征值之间的距离方法进行分类。</p>

<h3>有关K-近邻算法的问题</h3>

<p>优点：精度高，对异常值不敏感，无数据输入假定</p>

<p>缺点：计算复杂度较高，空间复杂度较高</p>

<p>适用数据范围：数值型和标称型</p>

<h3>工作原理</h3>

<p>存在一个样本数据集合，即训练样本集，并且训练样本中的每一个数据都存在标签，我们可以清楚地知道每一个数据条目其与对应分类的所属关系。</p>

<p>然后在接受没有标记的新数据输入时，将新数据的特征提取出来将其一一与训练样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似的（即所谓的最近邻）的分类标签来作为新数据的标签。</p>

<p>其中为了避免偶尔性和离群值造成的误差因此有了以样本数据集中前k个最相似的数据作为判别的参考的标准这种做法。<strong>通常k是不大于20的整数而且一般选取奇数作为k值</strong>（奇数可以在投票分类时避免出现等票的情况），最终的分类结果由k个样本的分类标签投票形成，出现最多的分类标签作为新数据的分类。</p>

<h3>一般算法流程</h3>

<ol>
<li>收集数据</li>
<li>准备数据：对数据进行清洗和结构化处理，使得数据可以进行距离计算</li>
<li>分析数据：提取相关特征</li>
<li>训练分类：knn算法并不需要训练</li>
<li>测试算法：计算错误率</li>
<li>使用算法：将新数据输入进行对应结构化之后，运行算法进行判定分类情况，并对后续的分类结果进行进一步处理应用</li>
</ol>


<h2>用KNN 制作简单的分类器</h2>

<p>使用数据：<a href="https://archive.ics.uci.edu/ml/datasets/Iris">UCI的鸢尾花数据集</a>
点击进入目标链接后:</p>

<blockquote><p>按照 Download Data Folder > iris.data 路径来下载指定数据集</p></blockquote>

<h4>准备数据</h4>

<p>在这里我们使用Python 的 Pandas 包以没有标题栏的csv文件的形式读入数据</p>

<p><strong>关键函数：read_csv</strong></p>

<pre><code class="python">
# loading libraries
import pandas as pd

# define column names
names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']

# loading training data
df = pd.read_csv('iris.data.txt', header=None, names=names)
df.head()
</code></pre>

<p><strong>输出</strong>：</p>

<table>
<thead>
<tr>
<th> index </th>
<th> sepal_length </th>
<th> sepal_width </th>
<th> petal_length </th>
<th> petal_width </th>
<th> class       </th>
</tr>
</thead>
<tbody>
<tr>
<td> 0     </td>
<td> 5.1          </td>
<td> 3.5         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 1     </td>
<td> 4.9          </td>
<td> 3.0         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 2     </td>
<td> 4.7          </td>
<td> 3.2         </td>
<td> 1.3          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 3     </td>
<td> 4.6          </td>
<td> 3.1         </td>
<td> 1.5          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 4     </td>
<td> 5.0          </td>
<td> 3.6         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
</tbody>
</table>


<h4>构建算法</h4>

<p><strong>距离函数</strong></p>

<p>我a们在上面的例子中把一个很重要的概念隐藏了起来，在选择一个数量k还只是小问题，更重要的是距离的计算方法。毕竟，当我们说“最近的k个点”时，这个“近”是怎么衡量的？</p>

<p>在数学中，一个空间上距离的严格定义如下：
设 M 为一个空间，M上的一个距离函数是一个函数<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <mrow>
  <mi>d</mi>
  <mo>:</mo>
  <mrow>
   <mrow>
    <mrow>
     <mi>M</mi>
     <mo>&#8290;</mo>
     <mi>x</mi>
     <mo>&#8290;</mo>
     <mi>M</mi>
    </mrow>
    <mo>-</mo>
   </mrow>
   <mo>&gt;</mo>
   <mi>R</mi>
  </mrow>
 </mrow>
</math>，满足</p>

<ul>
<li>d(x,y)≥0  ∀x,y∈M</li>
<li>d(x,y)=0⟺x=y</li>
<li>d(x,y)=d(y,x) ∀x,y∈M</li>
<li>d(x,z)≤d(x,y)+d(y,z) ∀x,y,z∈M
两个点 x,y 之间的距离就是<math xmlns='http://www.w3.org/1998/Math/MathML'
  mathematica:form='TraditionalForm'
  xmlns:mathematica='http://www.wolfram.com/XML/'>
<mrow>
<mi>d</mi>
<mo>(</mo>
<mrow>
 <mi>x</mi>
 <mo>,</mo>
 <mi>y</mi>
</mrow>
<mo>)</mo>
</mrow>
</math>。</li>
</ul>


<p>我们一般最常用的距离函数是欧氏距离，也称作<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <msub>
  <mi>L</mi>
  <mn>2</mn>
 </msub>
</math>距离。</p>

<p>如果
x=(x1,x2,…,xn) 和 y=(y1,y2,…,yn)是 n 维欧式空间 Rn 上的两个点，那它们之间的<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <msub>
  <mi>L</mi>
  <mn>2</mn>
 </msub>
</math>距离是</p>

<p><math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <mrow>
  <mrow>
   <msub>
    <mi>d</mi>
    <mn>2</mn>
   </msub>
   <mo>(</mo>
   <mrow>
    <mi>X</mi>
    <mo>,</mo>
    <mi>Y</mi>
   </mrow>
   <mo>)</mo>
  </mrow>
  <mo>&#63449;</mo>
  <msqrt>
   <mrow>
    <munderover>
     <mo movablelimits='true'>&#8721;</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msub>
        <mi>X</mi>
        <mi>i</mi>
       </msub>
       <mo>-</mo>
       <msub>
        <mi>Y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mo>&#8290;</mo>
     <msup>
      <mo>&#8202;</mo>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </msqrt>
 </mrow>
</math></p>

<p>由于Python 的scikit-learn包已经实现了KNN算法，因此我们在这里可以直接调用。</p>

<p>在<a href="http://scikit-learn.org/stable/index.html">scikit—learn</a>中，需要以matrix的形式和目标向量的形式来导入和训练数据。</p>

<p>因此在使用scikit-learn之前需要做额外的数据结构处理，同时还应该把原数据划分成训练数据和测试数据这样更加有利于我们下面的算法正确率评估。</p>

<pre><code class="python">
# loading libraries
import numpy as np
from sklearn.cross_validation import train_test_split

# create design matrix X and target vector y
X = np.array(df.ix[:, 0:4])     # end index is exclusive
y = np.array(df['class'])   # another way of indexing a pandas df

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
</code></pre>

<p>最后，我们根据划分好的数据集来构建真正的分类器，并用构建出来的分类器来进行数据拟合以及评估他的正确率。</p>

<pre><code class="python">
# loading library
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
# instantiate learning model (k = 3)
knn = KNeighborsClassifier(n_neighbors=3)

# fitting the model
knn.fit(X_train, y_train)

# predict the response
pred = knn.predict(X_test)

# evaluate accuracy
# print(y_test,pred)
print(metrics.accuracy_score(y_test, pred))
</code></pre>

<p><strong>输出</strong></p>

<blockquote><p>0.98</p></blockquote>

<p>由此可见，在适用的场合之下，KNN分类器的准确率也是可以达到一个较为理想的水平的。</p>
]]></content>
  </entry>
  
</feed>
