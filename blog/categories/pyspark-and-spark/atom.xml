<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: pyspark&spark | EdmondFrank's 时光足迹]]></title>
  <link href="https://edmondfrank.github.io/blog/categories/pyspark-and-spark/atom.xml" rel="self"/>
  <link href="https://edmondfrank.github.io/"/>
  <updated>2017-11-04T23:51:25+08:00</updated>
  <id>https://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习(四)之可视化工具的介绍与PySpark的结合使用示例]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/11/04/pythonzai-sparkshang-de-ji-qi-xue-xi-si-zhi-ke-shi-hua-gong-ju-de-jie-shao-yu-pysparkde-jie-he-shi-yong-shi-li/"/>
    <updated>2017-11-04T23:20:17+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/11/04/pythonzai-sparkshang-de-ji-qi-xue-xi-si-zhi-ke-shi-hua-gong-ju-de-jie-shao-yu-pysparkde-jie-he-shi-yong-shi-li</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习四之可视化工具的介绍与pyspark的结合使用示例">Python在Spark上的机器学习(四)之可视化工具的介绍与PySpark的结合使用示例</h1></p>

<h2 id="前言">前言</h2>




<p>在Python和Java的生态圈中，有许多可用的可视化库，但是在这篇文章中，我们主要来介绍一下matplotlib 和 Bokeh的使用。</p>




<p>首先，这两个库都是<a href="https://www.anaconda.com/">Anaconda</a>预装的。如果你是通过Anaconda来搭建的Python的科学计算环境的话，直接就可以通过import导入来使用这两个库了。</p>




<p>但是如果还没安装和配置好环境的朋友，可以自行参考<a href="http://matplotlib.org/index.html">Matplotlib</a>和<a href="https://bokeh.pydata.org/en/latest/">Bokeh</a>的官方站点的教程来下载配置环境。</p>




<blockquote>
  <p>注：这一类对各系统平台支持良好的库，一般安装流程也就无非两条pip命令，如： <br>
  python -mpip install -U pip <br>
  python -mpip install -U matplotlib <br>
  pip install bokeh <br>
  或 <br>
  conda install bokeh <br>
  所以各位读者也没有必要担心配置麻烦。</p>
</blockquote>




<h2 id="有关matplotlib和bokeh的介绍">有关matplotlib和bokeh的介绍</h2>




<h3 id="matplotlib">Matplotlib</h3>




<p><strong>Matplotlib</strong>是一个Python 2D绘图库，可以跨平台生成各种通用格式和适用于交互式环境的高质量图表。 Matplotlib可直接用于Python脚本，IPython shell，Jupyter以及Web应用程序服务器之中。 <br>
<strong>Matplotlib</strong>简化了许多繁琐的绘图操作，使得原本简单的图表在绘制上更加简单，而复杂的图表绘制也更容易上手。只需几行代码即可生成许多好看的图表。如，直方图、功率谱、条形图、错误图，散点图等。</p>




<p>官方绘图预览：</p>




<p><img src="http://matplotlib.org/_images/sphx_glr_simple_plot_0011.png" alt="enter image description here" title=""> <br>
<img src="http://matplotlib.org/_images/sphx_glr_histogram_features_0011.png" alt="enter image description here" title=""></p>




<p><img src="http://matplotlib.org/_images/sphx_glr_barchart_demo_0011.png" alt="enter image description here" title=""> <br>
<img src="http://matplotlib.org/_images/sphx_glr_pie_features_0011.png" alt="enter image description here" title=""></p>




<h3 id="bokeh">Bokeh</h3>




<p><strong>Bokeh</strong> (Bokeh.js) 是一个 Python 交互式可视化库，支持现代化 Web 浏览器，提供非常完美的展示功能。Bokeh 的目标是使用 D3.js 样式提供优雅，简洁新颖的图形化风格，同时提供大型数据集的高性能交互功能。Boken 可以快速的创建交互式的绘图，仪表盘和数据应用。</p>




<p>鉴于Bokeh强调的更多是一种交互式的绘图体验，在这里我就不貼静态图了，不过下面我会附上一些官方demo的例子，让大家感受下Bokeh的强大之处。</p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/stocks.html">趋势走向图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/iris.html">散点图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/texas.html">地域分布图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/boxplot.html">箱型图</a></p>




<h2 id="结合pyspark进行可视化分析">结合PySpark进行可视化分析</h2>




<h3 id="模块加载">模块加载</h3>




<p>以下实验均在Jupyter环境下进行 <br>
<strong>matplotlib</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">%matplotlib inline
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.style.use(<span class="hljs-string">'ggplot'</span>)</code></pre>




<p><strong>bokeh</strong></p>




<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> bokeh.charts <span class="hljs-keyword">as</span> chrt
<span class="hljs-keyword">from</span> bokeh.io <span class="hljs-keyword">import</span> output_notebook
output_notebook()</code></pre>




<h3 id="频率分布分析">频率分布分析</h3>




<p>频率分布图是最为简单有效的观察数据的分布情况的方法之一。</p>




<h4 id="读取数据">读取数据</h4>




<p>本文用到的数据文件依旧是上文所提及的信用欺诈检测的数据集，具体下载地址：<a href="http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz">这里</a></p>


<pre><code class="python">import pyspark.sql.types as typ
fraud = sc.textFile('/home/ef/Desktop/learningPySpark-master/ccFraud.csv')
header = fraud.first()
fraud = fraud \
.filter(lambda row: row != header) \
.map(lambda row: [int(elem) for elem in row.split(',')])
fields = [
*[
typ.StructField(h[1:-1], typ.IntegerType(), True)
for h in header.split(',')
]
]
schema = typ.StructType(fields)
fraud_df = spark.createDataFrame(fraud, schema)
hists = fraud_df.select('balance').rdd.flatMap(
lambda row: row
).histogram(20)

fraud_df.printSchema()
</code></pre>

<blockquote>
  <p>输出： <br>
  root <br>
   |– custID: integer (nullable = true) <br>
   |– gender: integer (nullable = true) <br>
   |– state: integer (nullable = true) <br>
   |– cardholder: integer (nullable = true) <br>
   |– balance: integer (nullable = true) <br>
   |– numTrans: integer (nullable = true) <br>
   |– numIntlTrans: integer (nullable = true) <br>
   |– creditLine: integer (nullable = true) <br>
   |– fraudRisk: integer (nullable = true)</p>
</blockquote>




<h4 id="绘制频率分布直方图">绘制频率分布直方图</h4>




<p><strong>matplotlib</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">data = {
<span class="hljs-string">'bins'</span>: hists[<span class="hljs-number">0</span>][:-<span class="hljs-number">1</span>],
<span class="hljs-string">'freq'</span>: hists[<span class="hljs-number">1</span>]
}
plt.bar(data[<span class="hljs-string">'bins'</span>], data[<span class="hljs-string">'freq'</span>], width=<span class="hljs-number">2000</span>)
plt.title(<span class="hljs-string">'Histogram of \'balance\''</span>)
plt.show()</code></pre>




<blockquote>
  <p>输出: <br>
  <img src="https://ooo.0o0.ooo/2017/10/27/59f2fbe5dc612.png" alt="mat_hist.png" title=""></p>
</blockquote>




<p><strong>bokeh</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">data = {
<span class="hljs-string">'bins'</span>: hists[<span class="hljs-number">0</span>][:-<span class="hljs-number">1</span>],
<span class="hljs-string">'freq'</span>: hists[<span class="hljs-number">1</span>]
}
b_hist = chrt.Bar(
data,
values=<span class="hljs-string">'freq'</span>, label=<span class="hljs-string">'bins'</span>,
title=<span class="hljs-string">'Histogram of \'balance\''</span>)
chrt.show(b_hist)</code></pre>




<blockquote>
  <p>输出: <br>
  <img src="https://ooo.0o0.ooo/2017/10/27/59f2fbe5e1f34.png" alt="bokeh_hist.png" title=""></p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习(三)之统计分析]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/10/27/pythonzai-sparkshang-de-ji-qi-xue-xi-san-zhi-tong-ji-fen-xi/"/>
    <updated>2017-10-27T18:39:42+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/10/27/pythonzai-sparkshang-de-ji-qi-xue-xi-san-zhi-tong-ji-fen-xi</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习三之统计分析">Python在Spark上的机器学习(三)之统计分析</h1></p>

<h2 id="背景">背景</h2>




<p>通常来说，一个完整使用机器学习建模解决问题的过程包含一下步骤：</p>




<ul>
<li>数据获取</li>
<li>数据预处理</li>
<li>数据统计分析</li>
<li>算法建模</li>
<li>训练</li>
<li>预测/分类</li>
</ul>




<p>这就意味着，在我们进行一般的数学建模或者挑选机器学习训练算法之前，应该先对数据进行清洗以及简单的统计分析，以便了解数据中显著的特征或者规律（虽然现在的机器学习方法，很多情况下根本不需要了解数据的意义，仅仅是通过堆叠特征就能获得一个可行的结果，但这显然离一个优秀的结果还是有一段距离的）。为了得到更加鲁棒的模型，以及了解数据背后的含义，我们这篇文章就来讲讲如何在PySpark上进行简单的统计分析。</p>




<h3 id="概念介绍">概念介绍</h3>




<p><strong>描述性统计分析</strong> <br>
这是一个统计学的概念，描述性统计是以揭示数据分布特性的方式汇总并表达定量数据的方法。主要包括数据的频数分析、数据的集中趋势分析、数据离散程度分析、数据的分布、以及一些基本的统计图形。特征括并表示定量数据，揭示数据分布的特征。 <br>
描述性统计是一类统计方法的汇总，作用是提供了一种概括和表征数据的有效且相对简便的方法。通常用图示法来表述，易于看懂，能发现质量特性值（总体）的分布状况、趋势走向的一些规律，便于采取措施。用于汇总和表征数据，通常是对数据进一步定量分析的基础，或是对推断性统计方法的有效补充。</p>




<h3 id="数据读取">数据读取</h3>




<p>本文使用的是一个信用欺诈检测的一个数据集，具体下载地址：<a href="http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz">这里</a></p>


<pre><code class="python">import PySpark.sql.types as typ
fraud = sc.textFile('data/ccFraud.csv')
header = fraud.first()
fraud = fraud.filter(lambda row:row!=header)\
.map(lambda row:[int(elem) for elem in row.split(',')])
fields = [
    *[
        typ.StructField(h[1:-1],typ.IntegerType(),True)
        for h in header.split(',')
    ]
]
schema = typ.StructType(fields)
fraud_df = spark.createDataFrame(fraud,schema=schema)
fraud_df.printSchema()
fraud_df.head()
</code></pre>

<blockquote>
  <p>输出结果 <br>
  root <br>
   |– custID: integer (nullable = true) <br>
   |– gender: integer (nullable = true) <br>
   |– state: integer (nullable = true) <br>
   |– cardholder: integer (nullable = true) <br>
   |– balance: integer (nullable = true) <br>
   |– numTrans: integer (nullable = true) <br>
   |– numIntlTrans: integer (nullable = true) <br>
   |– creditLine: integer (nullable = true) <br>
   |– fraudRisk: integer (nullable = true) <br>
  Out: <br>
  Row(custID=1, gender=1, state=35, cardholder=1, balance=3000, numTrans=4, numIntlTrans=14, creditLine=2, fraudRisk=0)</p>
</blockquote>




<p>在上面的代码中，我们读入了我们的数据集，以及创建了一个DataFrame，下面我们再进行一些简单的分析操作。</p>




<h3 id="简单统计分析">简单统计分析</h3>


<pre><code class="python">fraud_df.groupby('gender').count().show() #按照性别分类汇总


numerical = ['balance', 'numTrans', 'numIntlTrans']
desc = fraud_df.describe(numerical) 
##对balance，numTrans，numIntTrans进行描述性分析
desc.show()


# 计算balance值分布的偏度
fraud_df.agg({'balance': 'skewness'}).show()
</code></pre>

<blockquote>
  <p>输出结果 <br>
  +——+——-+ <br>
  |gender|  count| <br>
  +——+——-+ <br>
  |     1|6178231| <br>
  |     2|3821769| <br>
  +——+——-+ <br>
  +——-+—————–+——————+—————–+ <br>
  |summary|          balance|          numTrans|     numIntlTrans| <br>
  +——-+—————–+——————+—————–+ <br>
  |  count|         10000000|          10000000|         10000000| <br>
  |   mean|     4109.9199193|        28.9351871|        4.0471899| <br>
  | stddev|3996.847309737258|26.553781024523122|8.602970115863904| <br>
  |    min|                0|                 0|                0| <br>
  |    max|            41485|               100|               60| <br>
  +——-+—————–+——————+—————–+ <br>
  +——————+ <br>
  | skewness(balance)| <br>
  +——————+ <br>
  |1.1818315552993839| <br>
  +——————+</p>
</blockquote>




<p>上面我们使用了一些常用的统计分析函数，以及简单地了解了一下PySpark 聚合函数agg()的使用。通常地，常用的聚合函数还有avg() , count(), countDistinct() , max() 等。</p>




<h3 id="相关分析">相关分析</h3>




<p><strong>相关分析</strong>（correlation analysis），相关分析是研究现象之间是否存在某种依存关系，并对具体有依存关系的现象探讨其相关方向以及相关程度，是研究随机变量之间的相关关系的一种统计方法。通常两个变量之间存在的相关关系有：正相关、完全正相关、负相关、完全负相关、无相关。</p>




<p><strong>相关系数</strong>是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母 r 表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔逊相关系数。简单来说，相关系数是衡量两个变量间相关关系的指标。</p>




<p>在PySpark中计算两个变量之间的相关系数是非常简单的，往往只需要一条代码：</p>




<blockquote>
  <p>fraud_df.corr(‘balance’, ‘numTrans’)#计算balance和numTrans的相关系数</p>
</blockquote>




<p>输出</p>




<blockquote>
  <p>Out：0.0004452314017265385</p>
</blockquote>




<p>我们还可以通过下面的方法来创建一个相关矩阵。</p>


<pre><code class="python">n_numerical = len(numerical)
corr = []
for i in range(0, n_numerical):
    temp = [None] * i
    for j in range(i, n_numerical):
        temp.append(fraud_df.corr(numerical[i], numerical[j]))
    corr.append(temp)
</code></pre>

<blockquote>
  <p>输出结果; <br>
  [[1.0, 0.0004452314017265387, 0.0002713991339817875], [None, 1.0, -0.00028057128198165555], [None, None, 1.0]]</p>
</blockquote>




<p>正如输出结果所示，这个信用欺诈检测的数据集中的特征之间不存在过大的相关关系，基本全部都为相互独立关系。因此，在选择特征代入机器学习算法训练时，可以采用全部的变量。正也是统计分析的用处，可以帮助我们了解变量之间的线性相关关系，有利于帮住我们选取训练的特征变量。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ Python在Spark上的机器学习(二)之数据操作]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/10/22/pythonzai-sparkshang-de-ji-qi-xue-xi-er-zhi-shu-ju-cao-zuo/"/>
    <updated>2017-10-22T16:18:47+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/10/22/pythonzai-sparkshang-de-ji-qi-xue-xi-er-zhi-shu-ju-cao-zuo</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习二之数据操作">Python在Spark上的机器学习(二)之数据操作</h1></p>

<h2 id="pyspark">PySpark</h2>




<p>PySpark 是 Spark 为 Python 开发者提供的 API。 <br>
Spark是用Scala语言写成的，Scala把要编译的东西编译为Java虚拟机（JVM）的字节码（bytecode）。Spark的开源社区开发了一个叫PySpark的工具库。它允许使用者用Python处理RDD。这多亏了一个叫Py4J的库，它让Python可以使用JVM的对象（比如RDD）。  <br>
Pyspark Internals这篇<a href="https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals">wiki</a>里介绍了pyspark的实现机制，大体是下面这张图就可以表示：</p>




<p><img src="http://i.imgur.com/YlI8AqEl.png" alt="enter image description here" title=""></p>




<h2 id="resilient-distributed-datasets">Resilient Distributed Datasets</h2>




<p>说到Spark上的数据模式，一定不能少的就是Spark中的核心：RDD了。与许多专有的大数据处理平台不同，Spark建立在统一抽象的RDD之上，使得它可以以基本一致的方式应对不同的大数据处理场景，包括MapReduce，Streaming，SQL，Machine Learning以及Graph等。这即Matei Zaharia所谓的“设计一个通用的编程抽象（Unified Programming Abstraction）。这正是Spark让人着迷的地方。</p>




<p><strong>RDD 具体是什么呢？</strong> <br>
RDD，全称Resilient Distributed Datasets，又称弹性分布式数据集。是一个可容错的、并行的数据结构，可以让用户显示地将数据储存到磁盘和内存当中，并能控制数据的分区。</p>




<p>RDD本质上是一个内存数据集，在访问RDD时，指针只会指向与操作相关的部分。例如存在一个面向列的数据结构，其中一个实现为Int的数组，另一个实现为Float的数组。如果只需要访问Int字段，RDD的指针可以只访问Int数组，避免了对整个数据结构的扫描。</p>




<p>RDD将操作分为两类：transformation与action。无论执行了多少次transformation操作，RDD都不会真正执行运算，只有当action操作被执行时，运算才会触发。而在RDD的内部实现机制中，底层接口则是基于迭代器的，从而使得数据访问变得更高效，也避免了大量中间结果对内存的消耗。</p>




<h2 id="使用pyspark">使用Pyspark</h2>




<p>在按照系列的上一个教程搭建好环境后，在终端中直接输入pyspark就可以运行Python与Spark的交互式的shell了。</p>




<p>那么，下面我们就以一些简单的例子来使用pyspark。</p>




<h3 id="创建rdd">创建RDD</h3>


<pre><code class="python">data = sc.parallelize(
    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), 
     ('Amber', 9)])
data
</code></pre>

<p>输出：</p>




<blockquote>
  <p>ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:475</p>
</blockquote>




<h3 id="rdd对象转换成python对象">RDD对象转换成Python对象</h3>


<pre><code class="python">data_heterogenous = sc.parallelize([('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain','visited', 4504]]).collect()
data_heterogenous
</code></pre>

<p>输出：</p>




<blockquote>
  <p>[(‘Ferrari’, ‘fast’), {‘Porsche’: 100000}, [‘Spain’, ‘visited’, 4504]]</p>
</blockquote>




<h3 id="读取文件及统计词频">读取文件及统计词频</h3>




<p>首先word.txt文件内容如下：</p>




<blockquote>
  <p>The dynamic lifestyle <br>
  people lead nowadays <br>
  causes many reactions <br>
   in our bodies and <br>
   the one that is the <br>
   most frequent of all <br>
   is the headache. However so good</p>
</blockquote>


<pre><code class="python">from operator import add
lines = sc.textFile('word.txt')
counts = lines.flatMap(lambda x: x.split(' '))\
.map(lambda x : (x, 1))\
.reduceByKey(add)
output = counts.collect()
print(output)
for (word, count) in output:
    print("%s: %i" %(word, count))
</code></pre>

<p>输出：</p>




<blockquote>
  <p>[(”, 4), (‘good’, 1), (‘in’, 1), (‘is’, 2), (‘However’, 1), (‘of’, 1), (‘causes’, 1), (‘lifestyle’, 1), (‘The’, 1), (‘headache.’, 1), (‘reactions’, 1), (‘most’, 1), (‘frequent’, 1), (‘that’, 1), (‘all’, 1), (‘our’, 1), (‘dynamic’, 1), (‘nowadays’, 1), (‘so’, 1), (‘the’, 3), (‘people’, 1), (‘bodies’, 1), (‘many’, 1), (‘one’, 1), (‘and’, 1), (‘lead’, 1)] <br>
  …</p>
</blockquote>




<h2 id="dataframe">DataFrame</h2>




<p>DataFrameDataFrame是Spark推荐的统一结构化数据接口，是一个不可变的分布式数据集合，它结构与关系数据库中的表类似。</p>




<p>类似于Python Pandas DataFrame或R DataFrame，它能够让用户轻松处理结构化数据。</p>




<p>DataFrame还允许用户通过Spark SQL数据库或者采用一些函数式的方法查询及操作结构数据，下面我们就通过一些例子来了解和使用DataFrame。</p>




<h3 id="创建dataframes">创建DataFrames</h3>


<pre><code class="python">stringJSONRDD = sc.parallelize(("""
{ "id": "123",
"name": "Katie",
"age": 19,
"eyeColor": "brown"
}""",
"""{
"id": "234",
"name": "Michael",
"age": 22,
"eyeColor": "green"
}""",
"""{
"id": "345",
"name": "Simone",
"age": 23,
"eyeColor": "blue"
}""")
)
swimmersJSON = spark.read.json(stringJSONRDD)
swimmersJSON.show()
swimmersJSON.createOrReplaceTempView("swimmersJSON") #这里创建了临时表
</code></pre>

<p>输出：</p>




<blockquote>
  <p>+—+——–+—+——-+ <br>
  |age|eyeColor| id|   name| <br>
  +—+——–+—+——-+ <br>
  | 19|   brown|123|  Katie| <br>
  | 22|   green|234|Michael| <br>
  | 23|    blue|345| Simone| <br>
  +—+——–+—+——-+</p>
</blockquote>




<h3 id="dataframe的简单内容及类型查询">DataFrame的简单内容及类型查询</h3>


<pre><code class="python">spark.sql("select * from swimmersJSON").collect()
swimmersJSON.printSchema() #显示表数据的类型
</code></pre>

<p>输出：</p>




<blockquote>
  <p>[Row(age=19, eyeColor=’brown’, id=’123’, name=’Katie’), <br>
   Row(age=22, eyeColor=’green’, id=’234’, name=’Michael’), <br>
   Row(age=23, eyeColor=’blue’, id=’345’, name=’Simone’)] <br>
   root <br>
   |– age: long (nullable = true) <br>
   |– eyeColor: string (nullable = true) <br>
   |– id: string (nullable = true) <br>
   |– name: string (nullable = true)</p>
</blockquote>




<h3 id="指定数据储存及处理的类型">指定数据储存及处理的类型</h3>


<pre><code class="python"># Import types
from pyspark.sql.types import *
# Generate comma delimited data
stringCSVRDD = sc.parallelize([
(123, 'Katie', 19, 'brown'),
(234, 'Michael', 22, 'green'),
(345, 'Simone', 23, 'blue')
])
# Specify schema
schema = StructType([
StructField("id", LongType(), True),
StructField("name", StringType(), True),
StructField("age", LongType(), True),
StructField("eyeColor", StringType(), True)
])
# Apply the schema to the RDD and Create DataFrame
swimmers = spark.createDataFrame(stringCSVRDD, schema)
# Creates a temporary view using the DataFrame
swimmers.createOrReplaceTempView("swimmers")
swimmers.printSchema()
</code></pre>

<p>输出：</p>




<blockquote>
  <p>root <br>
   |– id: long (nullable = true) <br>
   |– name: string (nullable = true) <br>
   |– age: long (nullable = true) <br>
   |– eyeColor: string (nullable = true)</p>
</blockquote>




<p><strong>明明可以自动匹配储存类型，为什么我们还要手动指定类型呢？</strong> <br>
因为，在自动匹配类型的情况下，有时会将ID，Age等我们未来将要用来计算的数据以String的方式存储，这样就不利于我们对这些数据进行加减等运算，所以手动指定储存类型还是很有必要的。</p>




<h3 id="使用sql语句查询及操作数据">使用SQL语句查询及操作数据</h3>


<pre><code class="python">spark.sql("select count(1) from swimmers").show()
spark.sql("select id, age from swimmers where age = 22").show()
spark.sql(
"select name, eyeColor from swimmers where eyeColor like 'b%'").show()
</code></pre>

<p>输出：</p>




<blockquote>
  <p>+——–+ <br>
  |count(1)| <br>
  +——–+ <br>
  |       3| <br>
  +——–+</p>
  
  <p>+—+—+ <br>
  | id|age| <br>
  +—+—+ <br>
  |234| 22| <br>
  +—+—+</p>
  
  <p>+——+——–+ <br>
  |  name|eyeColor| <br>
  +——+——–+ <br>
  | Katie|   brown| <br>
  |Simone|    blue| <br>
  +——+——–+</p>
</blockquote>




<h2 id="小结">小结</h2>




<p>在这篇文章中我们可以看出，通过Pyspark结合RDD与DataFrames让我们可以用Python用上Spark平台上的分布式优势，也能够进一步加速和优化我们平时的数据操作。通过Spark导出的抽象层的API我们无需学过过于复杂和繁多的语法就能操作RDD上的数据。这篇文章的内容主要是为了后面用在用Python在Spark进行数据建模和机器学习所铺路，但受限于文章篇幅，还有十分多的函数和API无提及。所以有兴趣的读者可以阅读下<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark DataFrame的官方文档</a>深入了解一下。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习(一)之环境搭建]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/10/16/pythonzai-sparkshang-de-ji-qi-xue-xi-%5B%3F%5D-zhi-huan-jing-da-jian/"/>
    <updated>2017-10-16T14:07:29+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/10/16/pythonzai-sparkshang-de-ji-qi-xue-xi-[?]-zhi-huan-jing-da-jian</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习一之环境搭建">Python在Spark上的机器学习(一)之环境搭建</h1></p>

<p>前面已经介绍了不少机器学习的算法了，那么机器学习又该如何结合大数据一起使用么？</p>




<h2 id="常言道工欲善其事必先利其器">常言道：工欲善其事，必先利其器</h2>




<p>既然来结合大数据与机器学习，我们就不得不提Spark了。</p>




<p>首先，<a href="https://spark.apache.org/docs/latest/index.html">Apache Spark</a> 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</p>




<p>Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。</p>




<p>讲了这么多Spark的优点，那么现在我们就先开始来搭建一个Spark 集群环境吧！</p>




<h3 id="安装基础环境">安装基础环境</h3>




<p><strong>1. Java1.8环境搭建</strong>（下载JDK1.8的）：</p>




<blockquote>
  <p>下载页面： <br>
  <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>
</blockquote>




<p>安装过程可以参考<a href="http://www.linuxidc.com/Linux/2016-05/131348.htm">Linux公社给出的教程</a></p>




<p><strong>Ubuntu用户：</strong> <br>
Ubuntu用户可以通过添加PPA源再通过Apt来进行安装</p>




<blockquote>
  <p><span>$</span> sudo add-apt-repository ppa:webupd8team/java <br>
  <span>$</span> sudo apt-get update <br>
  <span>$</span> sudo apt-get install oracle-java8-installer</p>
</blockquote>




<p><strong>2. Scala环境搭建</strong></p>




<p>下载scala安装包：</p>




<pre class="prettyprint"><code class=" hljs avrasm">wget -O <span class="hljs-string">"scala-2.12.3.deb"</span> 
<span class="hljs-label">https:</span>//downloads<span class="hljs-preprocessor">.lightbend</span><span class="hljs-preprocessor">.com</span>/scala/<span class="hljs-number">2.12</span><span class="hljs-number">.3</span>/scala-<span class="hljs-number">2.12</span><span class="hljs-number">.3</span><span class="hljs-preprocessor">.deb</span>
或者
wget -O <span class="hljs-string">"scala-2.12.3.rpm"</span> <span class="hljs-string">"https://downloads.lightbend.com/scala/2.12.3/scala-2.12.3.rpm"</span>
</code></pre>




<p>安装deb/rpm包：</p>




<pre class="prettyprint"><code class=" hljs avrasm">rpm -ivh scala-<span class="hljs-number">2.12</span><span class="hljs-number">.3</span><span class="hljs-preprocessor">.rpm</span>
dpkg -i scala-<span class="hljs-number">2.12</span><span class="hljs-number">.3</span><span class="hljs-preprocessor">.deb</span></code></pre>




<p>增加SCALA_HOME</p>




<blockquote>
  <p>$ vim /etc/profile</p>
</blockquote>




<p>增加如下内容;</p>




<blockquote>
  <p>export SCALA_HOME=/usr/share/scala</p>
</blockquote>




<p>刷新配置</p>




<blockquote>
  <p>$ source /etc/profile</p>
</blockquote>




<h3 id="安装hadoop">安装Hadoop</h3>




<p><strong>1.下载二进制包：</strong> <br>
wget <a href="http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz">http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</a></p>




<p><strong>2.解压并移动至相应目录：</strong></p>




<p>我的习惯是将软件放置/opt目录下：</p>




<blockquote>
  <p>tar -xvf hadoop-2.7.3.tar.gz <br>
  mv hadoop-2.7.3 /opt</p>
</blockquote>




<p><strong>3.修改相应的配置文件：</strong></p>




<blockquote>
  <p><strong>（1）</strong> $ vim /etc/profile</p>
</blockquote>




<p>增加如下内容：</p>




<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-comment">#hadoop enviroment </span>
<span class="hljs-keyword">export</span> HADOOP_HOME=/opt/hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">3</span>/
<span class="hljs-keyword">export</span> PATH=<span class="hljs-string">"<span class="hljs-variable">$HADOOP_HOME</span>/bin:<span class="hljs-variable">$HADOOP_HOME</span>/sbin:<span class="hljs-variable">$PATH</span>"</span>
<span class="hljs-keyword">export</span> HADOOP_CONF_DIR=<span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop
<span class="hljs-keyword">export</span> YARN_CONF_DIR=<span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop</code></pre>




<blockquote>
  <p><strong>（2）</strong> <span>$</span> vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh</p>
</blockquote>




<p>修改JAVA_HOME 如下：</p>




<blockquote>
  <p>export JAVA_HOME=&lt;你的Java安装目录&gt;</p>
</blockquote>




<p>-</p>




<blockquote>
  <p><strong>（3）</strong> <span>$</span> vim $HADOOP_HOME/etc/hadoop/core-site.xml</p>
</blockquote>




<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
                <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
                <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>hdfs://master:9000<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
        <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>io.file.buffer.size<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>131072<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
       <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
                <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
                <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>/opt/hadoop-2.7.3/tmp<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
        <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>




<blockquote>
  <p><strong>（4）</strong> <span>$</span> vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml</p>
</blockquote>




<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:50090<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>file:/opt/hadoop-2.7.3/hdfs/name<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>file:/opt/hadoop-2.7.3/hdfs/data<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>




<blockquote>
  <p><strong>（5）</strong> <span>$</span> vim $HADOOP_HOME/etc/hadoop/mapred-site.xml</p>
</blockquote>




<p>复制template，生成xml：</p>




<blockquote>
  <p>cp mapred-site.xml.template mapred-site.xml</p>
</blockquote>




<p>内容：</p>




<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
 <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:10020<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:19888<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>




<p><strong>（6）</strong> <span>$</span> vim $HADOOP_HOME/etc/hadoop/yarn-site.xml</p>




<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-comment">&lt;!-- Site specific YARN configuration properties --&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
     <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
           <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
           <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8032<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
     <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
          <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8030<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
      <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8031<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
     <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8033<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
     <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
         <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8088<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
     <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span></code></pre>




<p>至此master节点的hadoop搭建完毕</p>




<p>再启动之前我们需要</p>




<p>格式化一下namenode</p>




<blockquote>
  <p>$ hadoop namenode -format</p>
</blockquote>




<h3 id="安装spark">安装Spark</h3>




<p><strong>下载文件：</strong></p>




<pre class="prettyprint"><code class=" hljs mathematica">wget -<span class="hljs-keyword">O</span> <span class="hljs-string">"spark-2.1.0-bin-hadoop2.7.tgz"</span> <span class="hljs-string">"http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz"</span></code></pre>




<p><strong>解压并移动至相应的文件夹：</strong></p>




<blockquote>
  <p>tar -xvf spark-2.1.0-bin-hadoop2.7.tgz <br>
  mv spark-2.1.0-bin-hadoop2.7 /opt</p>
</blockquote>




<p><strong>修改相应的配置文件：</strong></p>




<blockquote>
  <p><strong>（1）</strong> <span>$</span> vim /etc/profie</p>
</blockquote>




<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-comment">#Spark enviroment</span>
<span class="hljs-keyword">export</span> SPARK_HOME=/opt/spark-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin-hadoop2.<span class="hljs-number">7</span>/
<span class="hljs-keyword">export</span> PATH=<span class="hljs-string">"<span class="hljs-variable">$SPARK_HOME</span>/bin:<span class="hljs-variable">$PATH</span>"</span></code></pre>




<blockquote>
  <p><strong>（2）</strong> <span>$</span> vim $SPARK_HOME/conf/spark-env.sh</p>
</blockquote>




<p>-</p>




<blockquote>
  <p>cp spark-env.sh.template spark-env.sh</p>
</blockquote>




<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-comment">#配置内容如下：</span>
<span class="hljs-keyword">export</span> SCALA_HOME=/usr/share/scala
<span class="hljs-keyword">export</span> JAVA_HOME=&lt;你的Java安装目录&gt;
<span class="hljs-keyword">export</span> SPARK_MASTER_IP=master
<span class="hljs-keyword">export</span> SPARK_WORKER_MEMORY=<span class="hljs-number">1</span>g
<span class="hljs-keyword">export</span> HADOOP_CONF_DIR=/opt/hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">3</span>/etc/hadoop</code></pre>




<p>至此，我们大部分环境基本安装完毕！</p>




<h2 id="测试spark">测试Spark</h2>




<p>为了避免麻烦这里我们使用spark-shell以及本地的文件（非hdfs），做一个简单的worcount的测试。</p>




<pre class="prettyprint"><code class=" hljs livecodeserver">val <span class="hljs-built_in">file</span>=sc.textFile(<span class="hljs-string">"/home/ef/Desktop/Notes/wordcount_test"</span>)
val rdd = <span class="hljs-built_in">file</span>.flatMap(<span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-built_in">split</span>(<span class="hljs-string">" "</span>)).map(<span class="hljs-built_in">word</span> =&gt; (<span class="hljs-built_in">word</span>,<span class="hljs-number">1</span>)).reduceByKey(_+_)
rdd.collect()
rdd.foreach(println)</code></pre>




<p><strong>展示图：</strong> <br>
<img src="https://i.loli.net/2017/10/10/59dc80d01e9ff.png" alt="spark-shell.png" title=""></p>




<h2 id="小结">小结</h2>




<p>到此，我们在Spark上进行机器学习训练的环境，就搭建完毕了，下章我们再开始讲Spark中的数据结构与Python中的区别，以及结合Pyspark来进行数据处理。</p>

]]></content>
  </entry>
  
</feed>
