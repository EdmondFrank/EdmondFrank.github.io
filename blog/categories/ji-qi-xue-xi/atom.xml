<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | EdmondFrank's 时光足迹]]></title>
  <link href="https://edmondfrank.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="https://edmondfrank.github.io/"/>
  <updated>2017-09-05T21:22:18+08:00</updated>
  <id>https://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[如何构建一个朴素贝叶斯分类器]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/09/03/ru-he-gou-jian-%5B%3F%5D-ge-po-su-bei-xie-si-fen-lei-qi/"/>
    <updated>2017-09-03T13:22:23+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/09/03/ru-he-gou-jian-[?]-ge-po-su-bei-xie-si-fen-lei-qi</id>
    <content type="html"><![CDATA[<h1 id="如何构建一个朴素贝叶斯分类器">如何构建一个朴素贝叶斯分类器</h1>




<h3 id="概念">概念</h3>




<p><strong>朴素贝叶斯分类</strong>是一些使用概率论来进行分类的方法中的一种代表。之所以成为为<strong>朴素</strong>，是因为整个形式化过程只做了最原始，最简单的假设。其具体表现为：在分类过程中，我们都假设为特征条件都是<strong>相互独立</strong>的，不互相构成影响。而贝叶斯即说明该方法是基于贝叶斯定理的。</p>




<p>朴素贝叶斯是贝叶斯决策理论的一部分，对比其他分类方法而言，其<strong>优点</strong>有：在数据较少的情况下仍然有效，并且可以处理多类别的问题，而不仅仅是二分类。<strong>但</strong>也存在对输入数据的准备方式较为敏感的缺点。</p>




<h3 id="目前在各方面的应用">目前在各方面的应用</h3>




<ol>
<li>垃圾邮件分类，即著名的贝叶斯过滤器</li>
<li>文本自动归类</li>
<li>文本情感的分析，话题分析。</li>
</ol>




<h3 id="核心思想">核心思想</h3>




<p><img src="https://i.loli.net/2017/09/02/59aa6f0d7afce.png" alt="beyas.png" title=""></p>




<p>现在假设我们有一张如上所示的两类数据的统计参数。 <br>
我们现在用p1(x,y)来表示数据点(x,y)属于类别X（图中的蓝点）的概率，用p2(x,y)表示数据点属于类别Y（图中的黄点）的概率。那么对于一个新的数据点(x,y)，我们可以用以下的规则来判断它的类别：</p>




<ul>
<li><script type="math/tex" id="MathJax-Element-24">If p1(x,y) > p2(x,y) then (x,y) -> X</script></li>
<li><script type="math/tex" id="MathJax-Element-25">If p2(x,y) > p1(x,y) then (x,y) -> Y</script></li>
</ul>




<p>也就是说，我们更倾向于选择高概率所对应的类别。这就是<strong>贝叶斯决策理论</strong>的核心思想。用一句话总结即：选择具有最高概率的决策。</p>




<h3 id="使用条件概率进行分类">使用条件概率进行分类</h3>




<p>从上面的推理我们得到两个分类准则：</p>




<ul>
<li><script type="math/tex" id="MathJax-Element-61">If p1(x,y) > p2(x,y) then (x,y) -> X</script></li>
<li><script type="math/tex" id="MathJax-Element-62">If p2(x,y) > p1(x,y) then (x,y) -> Y</script></li>
</ul>




<p>但是，这两个准则只是为了简化描述整个分类问题，而真正来说，在使用贝叶斯方法进行分类时，我们需要计算和比较的应该是 p(X|x,y) 和 p(Y|x,y) ，它们所代表的意义是：在给定某个由（x，y）表示的数据点之后，它来自X类别的概率和它来自Y类别的概率分别是多少？知道这点之后，我们来重新定义我们的分类准则：</p>




<ul>
<li><script type="math/tex" id="MathJax-Element-63">If P(X|x,y) > P(Y|x,y) then (x,y) -> X</script></li>
<li><script type="math/tex" id="MathJax-Element-64">If P(X|x,y) < P(Y|x,y) then (x,y) -> Y</script></li>
</ul>




<p>然后，我们可以再通过贝叶斯公式的转换把未知的概率用已知的概率计算出来。</p>




<p><strong>使用实例：垃圾邮件判别</strong> <br>
首先，我们假设有以下的统计数据：</p>




<ul>
<li>在74封电子邮件中，有30封为垃圾邮件。</li>
<li>在以上的74封电子邮件中，有51封邮件包含“sex”一词。</li>
<li>其中，20封包含了“sex”一词的邮件被认为是垃圾邮件。</li>
</ul>




<p>知道了以上这些简单的前提条件之后，我们可以尝试通过贝叶斯方法来求解我们下一封收到包含“sex”一词的邮件是垃圾邮件的概率是多少？</p>




<p><script type="math/tex" id="MathJax-Element-65">P (\text{spam}|\text{sex})=\frac{P(spam) * P  (\text{sex}|\text{spam})}{P (\text{sex})}=\frac {(20\div30)* (30\div74)} {51\div74} =\frac{20}{51}=0.39</script></p>




<p>其中，公式中的<strong>spam</strong>代表垃圾邮件的意思（1937年7月5日，美国罐头肉制造商Jay Hormel发布以其名字命名的「Hormel Spiced Ham（荷美尔香料火腿）」，后来通过命名比赛改名为 SPAM(Spiced Pork and Ham)，有添加香料（Spices）的猪肉火腿罐头。至于为何 SPAM演变成垃圾邮件呢？有一说法是源于一部英国喜剧团（Monty Python）曾在一出讽刺剧「spam-loving vikings（爱吃肉罐头的维京人），剧中有对夫妻去餐厅用餐，妻子不想吃SPAM罐头，可是在餐厅里有一大群人，高声地唱讼赞美「SPAM」称颂肉罐头的美味多达120次，让其他的用餐客人无可奈何。从此 SPAM就成为「重复、毫无益处、喧宾夺主、令人厌烦邮件」的代名词。就像当年经济萧条，人们买不起鲜肉，而吃的SPAM 肉罐头一样,没有营养成分。）</p>




<p>上面我们得出了一个词对整篇文章的分类判断情况，接下来我们可以进一步扩展这个问题，通过再增加以下为已知前提：</p>




<ul>
<li>25封邮件包含“money”一词，而其中24封被认为是垃圾邮件。</li>
</ul>




<p>那么现在当收到一封同时包含“sex”，“money”这两个词的邮件，它为垃圾邮件的可能性又有多大呢？</p>




<p><script type="math/tex" id="MathJax-Element-66">\frac{P(\text{spam}) P(\text{money}|\text{spam}) P(\text{sex}|\text{spam}\cap  
   \text{money})}{P(\text{money}) P(\text{sex}|\text{monry})}</script></p>




<p>写到这里我们发现，当我们不断增加相关词进行判定时，上述的贝叶斯推理公式也将越来越复杂。这时，就要开始应用朴素思想了，我们假设“sex”和“money”两者出现的事件都相互独立（虽然现实情况往往并不是这样的，当出现一些广告相关的词词语时，“金钱”一词往往就有很大的几率出现在后文，但这里采用朴素的思想来近似在分类上也足够准确了），这样我们就可以把上面的公式简化成：</p>




<p><script type="math/tex" id="MathJax-Element-67">P(\text{spam}|\text{money},\text{sex})=\frac{P(\text{spam})  
   P(\text{money}|\text{spam}) P(\text{sex}|\text{spam})}{p(\text{money})  
   P(\text{sex})}</script></p>




<p>这样，假如我们需要对一封收到的电子邮件进行分类的话，我们只要计算出在给出邮件内的词汇的情况下，它为一封垃圾邮件的条件概率即可。当然，这里存在的一个较大的缺陷就是，当一些词语极度相关时，而我们通过假设他们相互独立计算出来的概率可能并不是那么准确。</p>




<h3 id="分类实现">分类实现</h3>


<pre><code class="ruby"># -*- encoding : utf-8 -*-
class BeyesClassifier::Base
  extend BeyesClassifier::Storage::ActAsStorable
  attr_reader :name
  attr_reader :word_list
  attr_reader :category_list
  attr_reader :training_count

  attr_accessor :tokenizer
  attr_accessor :language

  attr_accessor :thresholds
  attr_accessor :min_prob


  storable :version,:word_list,:category_list,:training_count,:thresholds,:min_prob

  # opts :
  # language
  # stemming : true | false
  # weight
  # assumed_prob
  # storage
  # purge_state ?

  def initialize(name, opts={})
    @version = BeyesClassifier::VERSION

    @name = name

    # This values are nil or are loaded from storage
    @word_list = {}
    @category_list = {}
    @training_count=0

    # storage
    purge_state = opts[:purge_state]
    @storage = opts[:storage] || BeyesClassifier::Base.storage
    unless purge_state
      @storage.load_state(self)
    else
      @storage.purge_state(self)
    end

    # This value can be set during initialization or overrided after load_state
    @thresholds = opts[:thresholds] || {}
    @min_prob = opts[:min_prob] || 0.0


    @ignore_words = nil
    @tokenizer = BeyesClassifier::Tokenizer.new(opts)

  end

  def incr_word(word, category)
    @word_list[word] ||= {}

    @word_list[word][:categories] ||= {}
    @word_list[word][:categories][category] ||= 0
    @word_list[word][:categories][category] += 1

    @word_list[word][:_total_word] ||= 0
    @word_list[word][:_total_word] += 1


    # words count by categroy
    @category_list[category] ||= {}
    @category_list[category][:_total_word] ||= 0
    @category_list[category][:_total_word] += 1

  end

  def incr_cat(category)
    @category_list[category] ||= {}
    @category_list[category][:_count] ||= 0
    @category_list[category][:_count] += 1

    @training_count ||= 0
    @training_count += 1

  end

  # return number of times the word appears in a category
  def word_count(word, category)
    return 0.0 unless @word_list[word] &amp;&amp; @word_list[word][:categories] &amp;&amp; @word_list[word][:categories][category]
    @word_list[word][:categories][category].to_f
  end

  # return the number of times the word appears in all categories
  def total_word_count(word)
    return 0.0 unless @word_list[word] &amp;&amp; @word_list[word][:_total_word]
    @word_list[word][:_total_word].to_f
  end

  # return the number of words in a categories
  def total_word_count_in_cat(cat)
    return 0.0 unless @category_list[cat] &amp;&amp; @category_list[cat][:_total_word]
    @category_list[cat][:_total_word].to_f
  end

  # return the number of training item
  def total_cat_count
    @training_count
  end

  # return the number of training document for a category
  def cat_count(category)
    @category_list[category][:_count] ? @category_list[category][:_count].to_f : 0.0
  end

  # return the number of time categories in wich a word appear
  def categories_with_word_count(word)
    return 0 unless @word_list[word] &amp;&amp; @word_list[word][:categories]
    @word_list[word][:categories].length
  end

  # return the number of categories
  def total_categories
    categories.length
  end

  # return categories list
  def categories
    @category_list.keys
  end

  # train the classifier
  def train(category, text)
    @tokenizer.each_word(text) {|w| incr_word(w, category) }
    incr_cat(category)
  end

  # classify a text
  def classify(text, default=nil)
    # Find the category with the highest probability
    max_prob = @min_prob
    best = nil

    scores = cat_scores(text)
    scores.each do |score|
      cat, prob = score
      if prob &gt; max_prob
        max_prob = prob
        best = cat
      end
    end

    # Return the default category in case the threshold condition was
    # not met. For example, if the threshold for :spam is 1.2
    #
    #    :spam =&gt; 0.73, :ham =&gt; 0.40  (OK)
    #    :spam =&gt; 0.80, :ham =&gt; 0.70  (Fail, :ham is too close)

    return default unless best

    threshold = @thresholds[best] || 1.0

    scores.each do |score|
      cat, prob = score
      next if cat == best
      return default if prob * threshold &gt; max_prob
    end

    return best
  end

  def save_state
    @storage.save_state(self)
  end

  class &lt;&lt; self
    attr_writer :storage

    def storage
      @storage = BeyesClassifier::InMemoryStorage.new unless defined? @storage
      @storage
    end

    def open(name)
      inst = self.new(name)
      if block_given?
        yield inst
        inst.save_state
      else
        inst
      end
    end
  end
end
</code></pre>

<p>完整代码实现可以参考<a href="https://github.com/alexandru/stuff-classifier"><strong>alexandru大神的github项目stuff-classifier</strong></a></p>




<h3 id="算法训练和使用">算法训练和使用</h3>


<pre><code class="ruby"># 训练函数
def train(category, text)
  each_word(text) {|w| increment_word(w, category) }
  increment_cat(category)
end

# 使用
classifier.train :spam, "Grow your penis to 20 inches in just 1 week"
classifier.train :ham,  "I'm hungry, no I don't want your penis"
</code></pre>

<p>由于本文主要的目标是讲解贝叶斯算法，所以为了方便分词，主要使用英文语料训练，对于中文邮件分类而言，只要采用分词库分词即可，其他部分不变。</p>




<h3 id="提高准确度与算法优化">提高准确度与算法优化</h3>




<h4 id="计算优化">计算优化</h4>




<p>由于浮点数计算的一些先天性问题，如计算效率慢，精度不准确等。因此我们可以用自然对数来代替原本概率的形式。 <br>
<script type="math/tex" id="MathJax-Element-3265">claaify(word_1,word_2,…word_n)=\text{argmax}\sum _{i=1}^n \frac{\log  
   \left(P\left(\left.\text{word}_i\right|\text{spam}\right)\right)}{\log  
   (e)}+\frac{\log (P(\text{spam}))}{\log (e)}</script></p>




<h4 id="拉普拉斯平滑">拉普拉斯平滑</h4>




<p>由于贝叶斯公式依赖的都是用已知概率求未知概率，但是在分类中无法避免的一个情况就是当我们计算<script type="math/tex" id="MathJax-Element-4127">P(word_i|spam)</script>即：在已有的训练的垃圾邮件样本中出现<script type="math/tex" id="MathJax-Element-4128">word_i</script>的概率是多少时，我们遇到了一个从未在训练样本中出现过的词，那么此时就会导致<script type="math/tex" id="MathJax-Element-4129">P(word_i|spam)=0</script>。这样就会影响整体的判别。</p>




<p>这样由于训练样本不足导致分类器整体质量大大下降的问题很常见，为了解决这个问题，我们可以引入<strong>Laplace校准</strong>（即我们要讲的拉普拉斯平滑公式），它的原理非常简单，给未出现特征值，赋予一个“小”的值而不是0。</p>




<p>具体平滑方法如下： <br>
假设离散型随机变量z有{1,2,…,k}个值，我们用<script type="math/tex" id="MathJax-Element-4130">\phi _i=P(z=i)</script>来表示每个值的概率。假设有m个训练样本中，z的观察值是<script type="math/tex" id="MathJax-Element-4131">\left\{z^1,\text{...},z^m\right\}</script>其中每一个观察值对应k个值中的一个。那么根据原来的估计方法可以得到</p>




<p><script type="math/tex" id="MathJax-Element-4132">\phi _j=\frac{\sum _{i=1}^m 1 \left\{z^i=j\right\}}{m}</script></p>




<p>简单来说就是<script type="math/tex" id="MathJax-Element-4133">z=j</script>出现的比例。</p>




<p>拉普拉斯平滑法将每个k值出现次数事先都加1，即假设他们都出现过一次。 <br>
那么修改后的表达式为：</p>




<p><script type="math/tex" id="MathJax-Element-4134">\phi _j=\frac{\sum _{i=1}^m 1 \left\{z^i=j\right\}+1}{m+k}</script></p>




<p>每个z=j的分子都加1，分母加k。可见<script type="math/tex" id="MathJax-Element-4135">\sum _{j=1}^k \phi _j=1</script>。</p>




<p>这样在保持总体事件发现概率比例基本不变的同时，又避免了0概率的问题。</p>




<h4 id="去除停用词">去除停用词</h4>




<p><strong>停用词</strong>是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为<strong>Stop Words</strong>（停用词）。这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。</p>




<p>通常意义上，Stop Words大致为如下两类：</p>




<ol>
<li>第一类是那些应用十分广泛的词，在Internet上随处可见，比如“Web”一词几乎在每个网站上均会出现，对这样的词搜索引擎无 法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率。</li>
<li>第二类就更多了，包括了语气助词、副词、介词、连接词等，通常自身并无明确的意义，只有将其放入一个完整的句子中才有一定作用，如常见的“的”、“在”之类。</li>
</ol>




<p>为了使得最后用于判别邮件类型的词向量能够更加贴近邮件本身真正想表达的含义，我们可以在读取扫描文本并生成词列表时，先<strong>剔除停用词</strong>，再进行下一步的概率计算。</p>




<p>常用中文停用词表： <br>
链接: <a href="https://pan.baidu.com/s/1o8hw5Cm">https://pan.baidu.com/s/1o8hw5Cm</a> 密码: crti</p>




<p>常用英文停用词表： <br>
链接: <a href="https://pan.baidu.com/s/1jIOOfgM">https://pan.baidu.com/s/1jIOOfgM</a> 密码: 7g95</p>




<h4 id="阈值选取">阈值选取</h4>




<p>假设我们计算出一封邮件的<script type="math/tex" id="MathJax-Element-4368">P(spam) = 0.6 </script>那么是否我们因为 <script type="math/tex" id="MathJax-Element-4369">P(spam) = 0.6 > P(not spam)=1-0.6=0.4</script> 就可以把它定义为垃圾邮件呢？显然这样做是不合理的，一个好的概率阈值应该是在训练及测试样本上使得发生误判情况最少时所取得的值。通常我们可以定义一个误差评判函数再通过不同阈值在样本上的评分来选取出最佳的阈值。简单实现就如下面的代码所示：</p>


<pre><code class="ruby">def classify(text, default=nil)
  # Find the category with the highest probability

  max_prob = 0.0
  best = nil

  scores = cat_scores(text)
  scores.each do |score|
    cat, prob = score
    if prob &gt; max_prob
      max_prob = prob
      best = cat
    end
  end

  # Return the default category in case the threshold condition was
  # not met. For example, if the threshold for :spam is 1.2
  #
  #    :spam =&gt; 0.73, :ham =&gt; 0.40  (OK)
  #    :spam =&gt; 0.80, :ham =&gt; 0.70  (Fail, :ham is too close)

  return default unless best
  threshold = @thresholds[best] || 1.0

  scores.each do |score|
    cat, prob = score
    next if cat == best
    return default if prob * threshold &gt; max_prob
  end
  return best
end
</code></pre>

<p>最后，写到这里，贝叶斯分类器构建的讲解基本就结束了。通观全篇，我们依旧有个问题并没有解决：朴素贝叶斯分类有一个限制条件，就是特征属性必须有条件独立或基本独立（实际上在现实应用中几乎不可能做到完全独立）。当这个条件成立时，朴素贝叶斯分类法的准确率是最高的，但不幸的是，现实中各个特征属性间往往并不条件独立，而是具有较强的相关性，这样就限制了朴素贝叶斯分类的能力。然而，这个问题也并不是无解的，贝叶斯分类中有一种更高级、应用范围更广的一种算法——<strong>贝叶斯网络（又称贝叶斯信念网络或信念网络）</strong>，信念网络在一定程度上使得模型更接近真实的实际情况，有兴趣的读者可以进一步深入了解。</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python实现Fisher判别分析]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/08/30/pythonshi-xian-fisherpan-bie-fen-xi/"/>
    <updated>2017-08-30T09:06:21+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/08/30/pythonshi-xian-fisherpan-bie-fen-xi</id>
    <content type="html"><![CDATA[<h1 id="python实现fisher-判别分析">Python实现Fisher 判别分析</h1>




<h2 id="fisher原理">Fisher原理</h2>




<p>费歇（Fisher）判别思想是投影，使多维问题化为一维问题来处理。选择一个适当的投影轴，使所有的样本点都投影在这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一类内的投影值所形成的类内距离差尽可能小，而不同类间的投影值所形成的类间距离差尽可能大。 <br>
<img src="https://i.loli.net/2017/09/01/59a90dda8790e.png" alt="" title=""></p>




<p><img src="https://i.loli.net/2017/09/01/59a9136f647d1.png" alt="enter image description here" title=""></p>




<p>这样如果我们想要同类样列的投影点尽可能接近，可以让同类样列投影点的协方差尽可能小，即<script type="math/tex" id="MathJax-Element-1">w^T \left(\sum _0 w\right)+w^T \left(\sum _1 w\right)</script>尽可能小;而欲使异类样列的投影点尽可能远离，可以让类中心之间的距离尽可能大，即<script type="math/tex" id="MathJax-Element-2">\left[\left[u_0 w^T-u_1 w^T\right]\right]</script>尽可能大。同时结合两者我们可以得到欲最大化的目标： <br>
<img src="https://i.loli.net/2017/09/01/59a9136f56af9.png" alt="enter image description here" title=""></p>




<p>(本文图片截取自<a href="https://book.douban.com/subject/26708119/">《机器学习》</a>周志华)</p>




<p><img src="https://i.loli.net/2017/09/01/59a9136f635c6.png" alt="enter image description here" title=""></p>




<p>有了上面的推理之后我们接下来就以DNA分类为例来实现一下Fisher线性判别。</p>




<h2 id="数据准备">数据准备</h2>


<pre><code class="python">from sklearn.cross_validation import train_test_split
dna_list = []
with open('dna2','r') as f:
    dna_list = list(map(str.strip,f.readlines()))
    f.close()
print(len(dna_list))
def generate_feature(seq):
    for i in seq:
        size = len(i)
        yield [
        chary.count(i,'a')/size,
        chary.count(i,'t')/size,
        chary.count(i,'c')/size,
        chary.count(i,'g')/size]

X = np.array(list(generate_feature(dna_list)),dtype=float)
y = np.ones(20)
y[10:]=2
X_train, X_test, y_train, y_test = train_test_split(X[:20], y, test_size=0.1)
print(X_train,'\n',y_train)
</code></pre>

<blockquote>
  <p>输出结果： <br>
  40 <br>
  [[ 0.2972973   0.13513514  0.17117117  0.3963964 ] <br>
   [ 0.35454545  0.5         0.04545455  0.1       ] <br>
   [ 0.42342342  0.28828829  0.10810811  0.18018018] <br>
   [ 0.35135135  0.12612613  0.12612613  0.3963964 ] <br>
   [ 0.27927928  0.18918919  0.16216216  0.36936937] <br>
   [ 0.21818182  0.56363636  0.14545455  0.07272727] <br>
   [ 0.20720721  0.15315315  0.20720721  0.43243243] <br>
   [ 0.3         0.5         0.08181818  0.11818182] <br>
   [ 0.2         0.56363636  0.17272727  0.06363636] <br>
   [ 0.27027027  0.06306306  0.21621622  0.45045045] <br>
   [ 0.32727273  0.5         0.02727273  0.14545455] <br>
   [ 0.23423423  0.10810811  0.23423423  0.42342342] <br>
   [ 0.29090909  0.64545455  0.          0.06363636] <br>
   [ 0.18181818  0.13636364  0.27272727  0.40909091] <br>
   [ 0.29090909  0.5         0.11818182  0.09090909] <br>
   [ 0.25454545  0.51818182  0.1         0.12727273] <br>
   [ 0.27433628  0.36283186  0.19469027  0.16814159] <br>
   [ 0.27027027  0.15315315  0.16216216  0.41441441]]  <br>
   [ 1.  2.  1.  1.  1.  2.  1.  2.  2.  1.  2.  1.  2.  1.  2.  2.  2.  1.]</p>
</blockquote>




<h2 id="fisher算法实现">Fisher算法实现</h2>


<pre><code class="python">
def cal_cov_and_avg(samples):
    """
    给定一个类别的数据，计算协方差矩阵和平均向量
    :param samples: 
    :return: 
    """
    u1 = np.mean(samples, axis=0)
    cov_m = np.zeros((samples.shape[1], samples.shape[1]))
    for s in samples:
        t = s - u1
        cov_m += t * t.reshape(4, 1)
    return cov_m, u1


def fisher(c_1, c_2):
    """
    fisher算法实现(请参考上面推导出来的公式，那个才是精华部分)
    :param c_1: 
    :param c_2: 
    :return: 
    """
    cov_1, u1 = cal_cov_and_avg(c_1)
    cov_2, u2 = cal_cov_and_avg(c_2)
    s_w = cov_1 + cov_2
    u, s, v = np.linalg.svd(s_w)  # 奇异值分解
    s_w_inv = np.dot(np.dot(v.T, np.linalg.inv(np.diag(s))), u.T)
    return np.dot(s_w_inv, u1 - u2)
</code></pre>

<h2 id="判别类型">判别类型</h2>


<pre><code class="python">
def judge(sample, w, c_1, c_2):
    """
    true 属于1
    false 属于2
    :param sample:
    :param w:
    :param center_1:
    :param center_2:
    :return:
    """
    u1 = np.mean(c_1, axis=0)
    u2 = np.mean(c_2, axis=0)
    center_1 = np.dot(w.T, u1)
    center_2 = np.dot(w.T, u2)
    pos = np.dot(w.T, sample)
    return abs(pos - center_1) &lt; abs(pos - center_2)


w = fisher(X_train[:10], X_train[10:20])  # 调用函数，得到参数w
pred = []
for i in range(20):
    pred.append( 1 if judge(X[i], w, X_train[:10], X_train[10:20]) else 2)   # 判断所属的类别
# evaluate accuracy
pred = np.array(pred)
print(y,pred)
print(metrics.accuracy_score(y, pred))
out = []
for i in range(20,40):
    out.append( 1 if judge(X[i], w, X_train[:10], X_train[10:20]) else 2)   # 判断所属的类别
print(out)
</code></pre>

<blockquote>
  <p>输出结果： <br>
  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2. <br>
    2.  2.] [1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1] <br>
  0.95 <br>
  [1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1]</p>
</blockquote>




<p>在这我们可以看出我们的Fisher算法在测试集中的误差率还算理想，误判率仅有5%。但是，我们可以看出其预测分类并不如其他KNN，SVM，等算法的预测效果。</p>




<p>最后，有关Fisher算法的介绍也就到此结束了！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-近邻算法-Part2]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/08/23/k-jin-lin-suan-fa-part2/"/>
    <updated>2017-08-23T12:19:17+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/08/23/k-jin-lin-suan-fa-part2</id>
    <content type="html"><![CDATA[<h1>K-近邻算法-Part2</h1>

<h2>使用交叉验证来调整k值</h2>

<p>通常来说,一个最优的KNN模型其k参数所对应的预估错误率应该是最低的。因此，在选定模型k值的时候应该反复尝试不同的k值在预估上的效果，对比其错误率。初学者在这里为了降低模型的误差通常会将全部样本数据也作为训练集一起代入模型进行训练。虽然这种做法在训练时确实能够有效降低误差，对现有数据进行更好的拟合。但是，同时带来的后果是：我们会将数据的各种无法避免的真实误差，如测量误差，抽样误差等也训练进了我们的模型之中，使得训练出来的模型在新数据或未知数据上的预估效果特别差，这种现象也被称为<strong>过拟合（overfitting）</strong>。</p>

<p>为了降低预估的错误率以及避免过拟合现象的发生，我们可以在某种意义下将<strong>原始数据(dataset)</strong>进行分组,一部分做为<strong>训练集(train set)</strong>,另一部分做为<strong>验证集(validation set or test set)</strong>，首先用训练集对分类器进行训练,再利用验证集来测试训练得到的<strong>模型(model)</strong>,以此来做为评价分类器的性能指标。这种方法也就是所谓的<strong>交叉验证（Cross Validation）</strong>。</p>

<p>而在交叉验证中，<strong>K折交叉验证（k-fold cross validation）</strong>是比较常用的。其主要思想是：初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</p>

<p><img src="https://kevinzakka.github.io/assets/k_fold_cv.jpg" alt="enter image description here" /></p>

<p>为了更好的理解k折交叉验证，我们继续沿用part1部分的训练数据，使用10折交叉验证的方法来调整我们的k值。</p>

<pre><code class="python">
import matplotlib.pyplot as plt
from sklearn.cross_validation import cross_val_score
# creating odd list of K for KNN
myList = list(range(1,50))

# subsetting just the odd ones
neighbors = list(filter(lambda x: x % 2 != 0, myList))

# empty list that will hold cv scores
cv_scores = []

# perform 10-fold cross validation
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# changing to misclassification error
MSE = [1 - x for x in cv_scores]

# determining best k
optimal_k = neighbors[MSE.index(min(MSE))]
print ("The optimal number of neighbors is %d" % optimal_k)

# plot misclassification error vs k
plt.plot(neighbors, MSE)
plt.xlabel('Number of Neighbors K')
plt.ylabel('Misclassification Error')
plt.show()
</code></pre>

<p>在上面的程序中，我们在1-50的奇数中选取k值，并根据k值训练模型进行预估验证。最后根据不同k值训练出去的模型的<strong>均方误差（Mean Squared Error, MSE）</strong>作出折线图，并求出使得MSE最小的最优k值。</p>

<blockquote><p>输出结果:</p>

<p>The optimal number of neighbors is 7
<img src="https://kevinzakka.github.io/assets/cv_knn.png" alt="enter image description here" /></p></blockquote>

<p>由此我们可以得出：在这个模型中，10折交叉验证告诉我们最优的k值是7。</p>

<h2>尝试自己实现KNN算法</h2>

<p>到目前为止，我们都是调用sklearn库中的KNN来完成分类任务。那么，下面我们来尝试自己实现一个简单的KNN算法并用它来分类我们之前的数据。</p>

<p>经过上一篇文章的介绍，我们可以知道KNN算法的关键是计算出新的<strong>待分类数据</strong>与现有的样本数据的<strong>“距离”</strong>，而其中较为常用的还是<strong>欧式距离（euclidean distance ）</strong>。然后提取出k个最相邻的点，并根据他们大多数点的分类属性来给待分类点进行归类。</p>

<p>因此核心计算代码我们可以这样写：</p>

<pre><code class="python">def predict(X_train, y_train, x_test, k):
    # create list for distances and targets
    distances = []
    targets = []

    for i in range(len(X_train)):
        # first we compute the euclidean distance
        distance = np.sqrt(np.sum(np.square(x_test - X_train[i, :])))
        # add it to list of distances
        distances.append([distance, i])

    # sort the list
    distances = sorted(distances)

    # make a list of the k neighbors' targets
    for i in range(k):
        index = distances[i][1]
        targets.append(y_train[index])

    # return most common target
    return Counter(targets).most_common(1)[0][0]
</code></pre>

<p>在上面的代码中，我们首先创建一个保存距离的数组，并在存储完计算出的待测点与各样本点的距离后对数组进行升序排列，然后取出前k个最接近待测点的样本点，返回其出现最多的分类标签。</p>

<p>然后接下来让我继续完成整个KNN算法。</p>

<pre><code class="python">def kNearestNeighbor(X_train, y_train, X_test, predictions, k):

    # loop over all observations
    for i in range(len(X_test)):
        predictions.append(predict(X_train, y_train, X_test[i, :], k))
</code></pre>

<p>使用我们上面得出最优的 k = 7作为参数生成模型并进行预估。</p>

<pre><code class="python"># making our predictions 
from sklearn.metrics import accuracy_score
predictions = []

kNearestNeighbor(X_train, y_train, X_test, predictions, 7)

# transform the list into an array
predictions = np.asarray(predictions)
#print(y_test,predictions)
# evaluating accuracy
#for i in range(predictions.size):
#    print(predictions.tolist()[i],list(y_test)[i])
accuracy = accuracy_score(y_test, predictions)
print('\nThe accuracy of our classifier is %d%%' % int(accuracy*100))
</code></pre>

<blockquote><p>输出结果：
The accuracy of our classifier is 98%</p></blockquote>

<p>到此，我们基本已经完成了一个类似于sklearn库中的KNN算法了，并且还有不错的准确率。</p>

<h2>小结</h2>

<p>最后，KNN算法介绍性文章到这就结束了。我们来总结一下KNN算法的优点与不足。</p>

<h4>优点</h4>

<ul>
<li>易于理解</li>
<li>无需训练</li>
<li>容易迁移至多分类情况</li>
</ul>


<h4>不足</h4>

<ul>
<li>计算量大，时间复杂度随数据规模增大而增大</li>
<li>分类情况容易受高频分类影响</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-近邻算法-Part1]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/08/14/k-jin-lin-suan-fa-part1/"/>
    <updated>2017-08-14T23:48:56+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/08/14/k-jin-lin-suan-fa-part1</id>
    <content type="html"><![CDATA[<h1 id="k-近邻算法-part1">K-近邻算法-Part1</h1>




<h2 id="概述">概述</h2>




<p>K-近邻算法，即K-近邻分类算法，简称KNN其通过采用测量不同的特征值之间的距离方法进行分类。</p>




<h3 id="有关k-近邻算法的问题">有关K-近邻算法的问题</h3>




<p>优点：精度高，对异常值不敏感，无数据输入假定</p>




<p>缺点：计算复杂度较高，空间复杂度较高</p>




<p>适用数据范围：数值型和标称型</p>




<h3 id="工作原理">工作原理</h3>




<p>存在一个样本数据集合，即训练样本集，并且训练样本中的每一个数据都存在标签，我们可以清楚地知道每一个数据条目其与对应分类的所属关系。</p>




<p>然后在接受没有标记的新数据输入时，将新数据的特征提取出来将其一一与训练样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似的（即所谓的最近邻）的分类标签来作为新数据的标签。</p>




<p>其中为了避免偶尔性和离群值造成的误差因此有了以样本数据集中前k个最相似的数据作为判别的参考的标准这种做法。<strong>通常k是不大于20的整数而且一般选取奇数作为k值</strong>（奇数可以在投票分类时避免出现等票的情况），最终的分类结果由k个样本的分类标签投票形成，出现最多的分类标签作为新数据的分类。</p>




<h3 id="一般算法流程">一般算法流程</h3>




<ol>
<li>收集数据</li>
<li>准备数据：对数据进行清洗和结构化处理，使得数据可以进行距离计算</li>
<li>分析数据：提取相关特征</li>
<li>训练分类：knn算法并不需要训练</li>
<li>测试算法：计算错误率</li>
<li>使用算法：将新数据输入进行对应结构化之后，运行算法进行判定分类情况，并对后续的分类结果进行进一步处理应用</li>
</ol>




<h2 id="用knn-制作简单的分类器">用KNN 制作简单的分类器</h2>




<p>使用数据：<a href="https://archive.ics.uci.edu/ml/datasets/Iris">UCI的鸢尾花数据集</a> <br>
点击进入目标链接后: </p>




<blockquote>
  <p>按照 Download Data Folder &gt; iris.data 路径来下载指定数据集</p>
</blockquote>




<h4 id="准备数据">准备数据</h4>




<p>在这里我们使用Python 的 Pandas 包以没有标题栏的csv文件的形式读入数据</p>




<p><strong>关键函数：read_csv</strong></p>


<pre><code class="python">
# loading libraries
import pandas as pd

# define column names
names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']

# loading training data
df = pd.read_csv('iris.data.txt', header=None, names=names)
df.head()
</code></pre>

<p><strong>输出</strong>：</p>




<table>
<thead>
<tr>
  <th>index</th>
  <th>sepal_length</th>
  <th>sepal_width</th>
  <th>petal_length</th>
  <th>petal_width</th>
  <th>class</th>
</tr>
</thead>
<tbody><tr>
  <td>0</td>
  <td>5.1</td>
  <td>3.5</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>1</td>
  <td>4.9</td>
  <td>3.0</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>2</td>
  <td>4.7</td>
  <td>3.2</td>
  <td>1.3</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>3</td>
  <td>4.6</td>
  <td>3.1</td>
  <td>1.5</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>4</td>
  <td>5.0</td>
  <td>3.6</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
</tbody></table>




<h4 id="构建算法">构建算法</h4>




<p><strong>距离函数</strong></p>




<p>我a们在上面的例子中把一个很重要的概念隐藏了起来，在选择一个数量k还只是小问题，更重要的是距离的计算方法。毕竟，当我们说“最近的k个点”时，这个“近”是怎么衡量的？</p>




<p>在数学中，一个空间上距离的严格定义如下： <br>
设 M 为一个空间，M上的一个距离函数是一个函数<script type="math/tex" id="MathJax-Element-383">d:M\times M \rightarrow R</script>，满足：</p>




<ul>
<li><script type="math/tex" id="MathJax-Element-384">d(x,y)≥0  ∀x,y∈M</script></li>
<li><script type="math/tex" id="MathJax-Element-385">d(x,y)=0⟺x=y</script></li>
<li><script type="math/tex" id="MathJax-Element-386">d(x,y)=d(y,x) ∀x,y∈M</script></li>
<li><script type="math/tex" id="MathJax-Element-387">d(x,z)≤d(x,y)+d(y,z) ∀x,y,z∈M</script></li>
</ul>




<p>两个点 x,y 之间的距离就是<script type="math/tex" id="MathJax-Element-388">d(x,y)</script>。</p>




<p>我们一般最常用的距离函数是欧氏距离，也称作<script type="math/tex" id="MathJax-Element-389">L_2</script>距离。</p>




<p>如果 <br>
<script type="math/tex" id="MathJax-Element-390">x=(x1,x2,…,xn)</script> 和 <script type="math/tex" id="MathJax-Element-391">y=(y1,y2,…,yn)</script>是 n 维欧式空间 Rn 上的两个点，那它们之间的<script type="math/tex" id="MathJax-Element-392">L_2</script>距离是</p>




<p><script type="math/tex" id="MathJax-Element-393">d_2(X,Y)=\sqrt{\sum _{i=1}^n \left(X_i-Y_i\right){}^2}</script></p>




<p>由于Python 的scikit-learn包已经实现了KNN算法，因此我们在这里可以直接调用。</p>




<p>在<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>中，需要以matrix的形式和目标向量的形式来导入和训练数据。</p>




<p>因此在使用scikit-learn之前需要做额外的数据结构处理，同时还应该把原数据划分成训练数据和测试数据这样更加有利于我们下面的算法正确率评估。</p>


<pre><code class="python">
# loading libraries
import numpy as np
from sklearn.cross_validation import train_test_split

# create design matrix X and target vector y
X = np.array(df.ix[:, 0:4])     # end index is exclusive
y = np.array(df['class'])   # another way of indexing a pandas df

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
</code></pre>

<p>最后，我们根据划分好的数据集来构建真正的分类器，并用构建出来的分类器来进行数据拟合以及评估他的正确率。</p>


<pre><code class="python">
# loading library
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
# instantiate learning model (k = 3)
knn = KNeighborsClassifier(n_neighbors=3)

# fitting the model
knn.fit(X_train, y_train)

# predict the response
pred = knn.predict(X_test)

# evaluate accuracy
# print(y_test,pred)
print(metrics.accuracy_score(y_test, pred))
</code></pre>

<p><strong>输出</strong></p>




<blockquote>
  <p>0.98</p>
</blockquote>




<p>由此可见，在适用的场合之下，KNN分类器的准确率也是可以达到一个较为理想的水平的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习概述-科普向]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/08/06/ji-qi-xue-xi-gai-shu-ke-pu-xiang/"/>
    <updated>2017-08-06T23:02:41+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/08/06/ji-qi-xue-xi-gai-shu-ke-pu-xiang</id>
    <content type="html"><![CDATA[<h1>机器学习概论-科普篇</h1>

<h2>什么是机器学习？</h2>

<p>机器学习是一门多领域交叉学科。专门研究计算机或其他软硬件设备怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有知识结构使不断改善自身的性能。</p>

<h2>机器学习的应用领域</h2>

<p>机器学习是人工智能研究的核心内容。它的应用已遍及人工智能的各个分支。如：专家系统，自动推理，自然语言处理，模式识别，计算机视觉，智能机器人等领域。</p>

<h2>机器学习与数据挖掘的区别</h2>

<p>机器学习在数据挖掘中被大量使用，其技术内涵几乎通用，可以看作同一座山峰在不同角度下的侧影。</p>

<h2>机器学习与统计学的关系</h2>

<p>机器学习和统计学是非常接近的两个领域。根据 Michael I. Jordan在机器学习领域的理念，从方法论原则到理论工具，在统计学领域是有一段很长的史前史。他也建议数据科学这一术语作为全部领域的前置。 Leo Breiman区别两个统计学的模型：数据模型和算法模型，在算法模型中意味着或多或少包含着机器学习的算法，比如随机森林（Random forest）。 一些统计学家已经采纳了机器学习中的一些做法，引申出了一个联结领域&mdash;&ndash;统计学习。</p>

<h2>机器学习方法</h2>

<p><strong>决策树学习：</strong>决策树学习使用了一个决策树作为预测性模型，映射一个对象的观察结果给其目标价值一个推论。</p>

<p><strong>关联规则学习:</strong>是一种用来在大型数据库中发现变量之间的有趣联系的方法,例如频繁模式挖掘。</p>

<p><strong>人工神经网络：</strong>一个人工神经网络学习（ANN）算法，通常被称为神经网络（NN），是一个由生物的神经网络所激发出的一个算法。计算结构是由联结的人工神经元组所构成，通过联结式的方法来传递信息和计算。现代神经网络是非线性的统计学数据模型工具。它们通常被用来在输入和输出之间模拟复杂关系，找到数据中的关系，或者在观测变量中从不知道的节点捕获统计学结构。</p>

<p><strong>深度学习：</strong>个人不能承受硬件的价格和GPU的发展推动了这些年深度学习的进步，深度学习是由人工神经网络中的多个隐藏层组成的。这条道路试图去模拟人脑的过程，光、声进入视觉和听觉。一些成功的应用有计算机视觉和演讲识别。</p>

<p><strong>归纳逻辑编程：</strong>归纳逻辑编程（ILP）是一门用逻辑编程控制规则的学科，它使用统一的表示法来处理输入样例，背景知识和假说。给定已知的背景知识的编码和一组被表示为事实的逻辑数据库的示例，ILP系统将派生出一个假设的逻辑程序，该程序包含所有积极的和没有负面的示例。归纳编程是一个相关的领域，它考虑任何一种表示假设(而不仅仅是逻辑编程)的编程语言，例如函数式编程。</p>

<p><strong>支持向量机：</strong>支持向量机是一系列关于监督学习在分类和回归上的应用。给出训练样本的数据集，每一个标记属于两类中的一类，一个SVM训练算法构成了一个模型，可以用来预测一个新的样本是否进入一个类别或者是另一个。</p>

<p><strong>集群：</strong>集群分析是将一组观察结果分配到子集(称为集群)，这样，同一集群中的观察与一些预先确定的标准或标准相似，而来自不同集群的观察则不同。不同的聚类技术对数据的结构作出不同的假设，通常由一些相似性度量定义，并通过内部紧度(相同集群的成员之间的相似性)和不同的集群之间的分离来评估。其他方法基于估计的密度和图连通性。摘要聚类是一种非引导性学习的方法，是一种统计数据分析的常用技术。</p>

<p><strong>贝叶斯网络：</strong>一个贝叶斯网络，信任网络或者有向无环图模型是一个概率性图的模型，它通过有向无环图代表了一系列的随机变量和他们的条件独立性。举例，一个贝叶斯网络代表着疾病和症状可能的关系。给出症状，网络可以被用来计算疾病出现的可能性。有效的算法存在于执行推理和学习的过程中。</p>

<p><strong>增强学习：</strong>增强学习关心代理人如何在一个环境中采取行动，从而最大化一些长期受益的概念。增强学习算法尝试去寻找一些策略，映射当前世界的状态给代理在这些状态中应该采取的行动。</p>

<p><strong>相似度量学习：</strong>在这个问题中，学习机被给予了很多对相似或者不相似的例子。它需要去学习一个相似的函数，以用来预测一个新的对象是否相似。它有时被用到推荐系统中。</p>

<p><strong>遗传算法：</strong>遗传算法是一种启发式搜索，它模仿自然选择的过程，并且使用一些突变和变向来生成新的基因型，以找到好的情况解决问题。在机器学习中，遗传算法在20世纪80年代和90年代使用过。反之，机器学习技术被用来提高遗传和进化算法的表现。</p>

<p><strong>基于规则的机器学习：</strong>基于规则的机器学习是任何机器学习方法的通用术语，它可以识别、学习或发展规则来存储、操作或应用知识。基于规则的机器学习者的定义特征是一组关系规则的标识和利用，这些规则集合了系统所捕获的知识。这与其他机器学习者形成鲜明对比，他们通常会识别出一种特殊的模型，这种模型可以普遍应用于任何实例，以便做出预测。基于规则的机器学习方法包括学习分类器系统、关联规则学习和人工免疫系统。</p>

<h2>机器学习应用场景</h2>

<h3>活跃的领域：</h3>

<ul>
<li>数据分析</li>
<li>数据挖掘。</li>
<li>图像和语音识别</li>
<li>智能机器，机器人，人机对话，电脑博弈。</li>
</ul>


<h3>推荐系统：</h3>

<ul>
<li>基于物品的协同过滤</li>
<li>频繁模式挖掘</li>
</ul>


<h3>贝叶斯分类器：</h3>

<ul>
<li>垃圾邮件过滤</li>
<li>网页自动分类：自动化门户系统</li>
<li>评论自动分析</li>
</ul>


<h3>决策树</h3>

<ul>
<li>量化交易</li>
<li>智能博弈</li>
<li>局面标准化</li>
<li>局面评估函数</li>
<li>棋谱学习</li>
</ul>


<h3>神经网络和深度学习</h3>

<ul>
<li>语音识别,图像识别</li>
<li>图形识别：</li>
<li>车牌识别</li>
<li>指纹，虹膜纹识别</li>
<li>脸像识别</li>
<li>动态图像识别</li>
<li>小波分析</li>
</ul>


<h2>机器学习常用软件</h2>

<p>常用软件列表：</p>

<ul>
<li>R（及其扩展包）</li>
<li>Weka（Waikato Environment for Knowledge Analysis）</li>
<li>Matlab</li>
<li>Python,numpy,matplotlib,sklearn,tensorflow</li>
</ul>


<h2>代表性算法</h2>

<h3>回归预测及降维技术：</h3>

<ul>
<li>线性回归</li>
<li>Logistic回归</li>
<li>主成分分析</li>
<li>因子分析</li>
<li>岭回归</li>
<li>LASSO</li>
</ul>


<h3>分类器:</h3>

<ul>
<li>决策树</li>
<li>朴素贝叶斯</li>
<li>贝叶斯信念网络</li>
<li>支持向量机(SVM)</li>
<li>提升分类器准确率的Adaboost和随机森林算法</li>
</ul>


<h3>聚类和孤立点判别</h3>

<ul>
<li>Kmeans聚类</li>
</ul>


<h3>人工神经网路及深度学习</h3>

<ul>
<li>CNN</li>
<li>RNN</li>
</ul>


<p>&hellip;</p>
]]></content>
  </entry>
  
</feed>
